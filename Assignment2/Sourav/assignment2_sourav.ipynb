{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0b933634",
   "metadata": {
    "tags": []
   },
   "source": [
    "# IDS Assignment Part 2 - <font color=\"red\"><h7>Deadline: 23/01/2023 23:59</h7></font>\n",
    "This is the second part of the assignment in IDS 2022/2023. \n",
    "Please use this Jupyter notebook to work on the questions posed in the assignment. When you are done, upload the notebook in Moodle at the designated activity. In addition to the _Jupyter notebook_, please submit _one zip-file_ containing your screenshots for Question 7. \n",
    "\n",
    "Give your commented Python code and answers in the corresponding provided cells. Make sure to answer all questions in a clear and explicit manner and discuss your outputs. _Please do not change the general structure of this notebook_. You can, however, add additional markdown or code cells if necessary. <b>Please DO NOT CLEAR THE OUTPUT of the notebook you are submitting! </b>\n",
    "\n",
    "<font color=\"red\"> *Please make sure to include the names and matriculation numbers of all group members in the slot provided below.* </font> If a name or a student id is missing, the student will not receive any points.\n",
    "\n",
    "Hint 1: While working on the assignment, you will get a better understanding of the dataset. Feel free to generate additional results and visualizations to support your answers. For example, this might be useful regarding data modification, data simplification, or output interpretation. <font color=\"red\">Ensure that all your claims are supported.</font>\n",
    "\n",
    "Hint 2: <font color=\"red\">Plan your time wisely. </font> A few parts of this assignment may take some time to run. It might be necessary to consider time management when you plan your group work. Also, do not attempt to upload your assignment at the last minute before the deadline. This often does not work, and you will miss the deadline. Late submissions will not be considered.\n",
    "\n",
    "Hint 3: RWTHmoodle allows multiple submissions, with every new submission overwriting the previous one. <b>Partial submissions are therefore possible and encouraged. </b> This might be helpful in case of technical issues with RWTHMoodle, which may occur close to the deadline."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "76db0f12",
   "metadata": {},
   "source": [
    "<font color=\"red\"><b>Student Names and IDs:\n",
    "    \n",
    "    1. Sourav Kulkarni 417735\n",
    "    \n",
    "    2. \n",
    "    \n",
    "    3."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7803d7da-69e1-412a-8a9f-c04ceb96a0f2",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Question 1: Preprocessing (20 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3c5e97f-99a5-41d4-92a8-e9888bf4f4c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "### Sklearn\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.impute import KNNImputer"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3ffab9bb-d08f-44dd-ad3f-ac3dc8e2184c",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "In this question, we consider a dataset documenting the Ski Resorts in Europe (**ski_resorts.csv**).\n",
    "Each row contains some information about the Ski resort.\n",
    "You can find a short description for each column:\n",
    "\n",
    "| Column | Description |\n",
    "| ------ | ----------- |\n",
    "| Resort | The name of the ski & snowboard resort. |\n",
    "| Country | The country in which the resort is located. |\n",
    "| HighestPoint | The highest mountain point at the ski resort.   |\n",
    "| LowestPoint | The lowest possible point to ski at the ski resort.  |\n",
    "| DayPassPriceAdult | The price shows what it costs for 1 adult for 1 day in the main season in Euro. |\n",
    "| BeginnerSlope | The total amount of “beginner” slopes in kilometer at the resort. “Beginner slopes” contains “children”, “blue” and, “green” slopes. |\n",
    "| IntermediateSlope | The total amount of “intermediate” slopes in kilometer at the resort. “Intermediate slopes” contains “red” slopes. |\n",
    "| DifficultSlope | The total amount of “difficult” slopes in kilometer at the resort. “Difficult slopes” contains “black”, “advanced”, and ”expert” slopes. |\n",
    "| TotalSlope | The sum of “beginner slopes” + “intermediate slopes” + “difficult slopes” |\n",
    "| Snowparks | Does the resort have one or more snowparks, or not? |\n",
    "| NightSki | Does the resort offer skiing on illuminated slopes? |\n",
    "| SurfaceLifts | The amount of lifts in this category: T-bar, Sunkidslift, Rope lifts, and people mower. |\n",
    "| ChairLifts | The total amount of chairlifts. |\n",
    "| GondolaLifts | The amount of lifts in this category: Gondola, Train lifts, Funicular, Combined gondola and chairlifts, Helicopter lifts, Snowcats, and Aerial tramways. |\n",
    "| TotalLifts | The sum of “surface lifts etc” + “gondola etc” + “chairlifts etc.” |\n",
    "| LiftCapacity | How many passengers can the lift system at the ski resort mowe in one hour? |\n",
    "| SnowCannons  |The total amount of snow cannons at the ski resort.|"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e7bd4f6-32a5-4e9d-9339-00556a268506",
   "metadata": {
    "tags": []
   },
   "source": [
    "### a) Loading the Data and Initial Quality Investigation (2.5pts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82c39c84-0399-4369-ad4d-c657c08aefba",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### **a(i)** \n",
    "Load the dataset into a dataframe `df`. <font color='red'>Use the first column as index for your dataframe</font>. Ensure that the index is valid, that is, it should not contain any duplicate entries. \n",
    "\n",
    "\n",
    "\n",
    "**In the subsequent questions, only modify the dataframe `df` if explicitly requested. However, you can always create working copies.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "897a95f5-c50d-4240-b357-65ff937034ba",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# your code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca4b69ca-8603-4cf9-8e6a-32bea8b70a32",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-07T10:01:29.281703Z",
     "iopub.status.busy": "2022-12-07T10:01:29.281301Z",
     "iopub.status.idle": "2022-12-07T10:01:29.305867Z",
     "shell.execute_reply": "2022-12-07T10:01:29.304586Z",
     "shell.execute_reply.started": "2022-12-07T10:01:29.281673Z"
    },
    "tags": []
   },
   "source": [
    "#### **a(ii)** \n",
    "Show the data types of the dataframe columns as well as the first 5 rows. On the first sight, are there any data type problems (e.g., numerical columns having a non-numerical data type)?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a2e18e6-f590-449d-891f-f5684fdb23e2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# your code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34febc32-6c1a-45ec-96c5-f81dda43ba7b",
   "metadata": {},
   "source": [
    "**Your Answer:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8ff406b-5ec1-4b26-8abb-5b5a91643841",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### **a(iii)** \n",
    "To improve performance and memory usage (in particular for large datasets) it is important to use **categorical** columns whenever suitable.\n",
    "Are there any categorical column candidates? Explain your answer. \\\n",
    "Afterward, convert the column(s) in `df` into categorical column(s)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afa75081-bf63-4e85-94c2-483d9ae4118e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# your code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "131c3379-bb44-42d0-9c4f-9d1e4c2d38b6",
   "metadata": {},
   "source": [
    "**Your Answer:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0222a54c-f455-4b78-bae1-33bc7d79edf3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-07T16:58:43.496952Z",
     "iopub.status.busy": "2022-12-07T16:58:43.496547Z",
     "iopub.status.idle": "2022-12-07T16:58:43.506339Z",
     "shell.execute_reply": "2022-12-07T16:58:43.504499Z",
     "shell.execute_reply.started": "2022-12-07T16:58:43.496921Z"
    },
    "tags": []
   },
   "source": [
    "### b) Handling Missing Values & Encoding (17.5pts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "279b17dc-acae-4012-b83b-2bae628a8fb4",
   "metadata": {},
   "source": [
    "In the following task, you can assume that every NAN entry in the dataframe is actually a missing value. This can partially be justified by the fact that pandas did not have problems inferring the \"proper\" datatypes (e.g., a string indicating a missing number in a number column would result in pandas parsing an object column) and your subsequent check of the data types. Therefore, you can use `df.isna()` as a proxy indicator for missing values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a760130-8c4b-46b6-b96f-76dcea95d28e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-07T16:59:35.630762Z",
     "iopub.status.busy": "2022-12-07T16:59:35.630357Z",
     "iopub.status.idle": "2022-12-07T16:59:35.648425Z",
     "shell.execute_reply": "2022-12-07T16:59:35.646703Z",
     "shell.execute_reply.started": "2022-12-07T16:59:35.630730Z"
    },
    "tags": []
   },
   "source": [
    "#### **b(i)** \n",
    "Simply discarding missing entries is usually not a good idea. Therefore, you should first analyze the number of missing values and check for patterns of missing values. \n",
    "\n",
    "To this end, compute the following statistics on missing values:\n",
    "1. How many entries does the dataframe have? (To relate this to the number of entries missing)\n",
    "2. How many missing values do we have? What is the ratio i.e., \"number of missing values\"/\"number of entries of df\"?\n",
    "3. How many rows have at least a single missing value?\n",
    "4. Count the number of missing values per column.\n",
    "5. Count the number of missing values per row and aggregate them - i.e., show the number of rows that suffer from x missing values.\n",
    "6. What do you observe? Are there any rows containing missing values for the same set of columns? Can you identify potential patterns?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0b7c69e-8c4e-483e-9131-9b7f2936216c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# your code for 1. How many entries does the dataframe have? (To relate this to the number of entries missing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e10f80a7-4caf-4d0b-8cf8-c3148e466288",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# your code for 2. How many missing values do we have? What is the ratio i.e., \"number of missing values\"/\"number of entries of df\"?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4ecc58b-6563-4dd1-8727-4aa08a485961",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# your code for 3. How many rows have at least a single missing value?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55e8a812-67c1-4458-ac9e-41265cab70c3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# your code for 4. Count the number of missing values per column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "724f6928-edfe-4fec-b3fd-abfc6567d1ec",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# your code for 5. Count the number of missing values per row and aggregate them - i.e., show the number of rows that suffer from x missing values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fc8a782-1430-4e7c-835c-91b7e1806597",
   "metadata": {
    "tags": []
   },
   "source": [
    "**Your answer:** *(for 6. What do you observe? Are there any rows containing missing values for the same set of columns?)*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5467e51c-6ba1-47f6-a123-fd55076b8e45",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### **b(ii)**\n",
    "For the next step:\n",
    "\n",
    "1. Transform the categorical column(s) you identified in a(iii) into one-hot encoding format. \n",
    "2. Transform the columns \"Snowparks\" and \"NightSki\" in `df` into boolean data type, where \"Yes\" should be `True` and \"No\" should be `False`\n",
    "\n",
    "In the end, the original categorical column(s) should still be there. Additionally, there should be x number (x is the number of unique values) of one-hot encoding columns for each categorical column. Use the following naming convention for the new columns \"{name of the categorical column}_{unique value for that column}\" Also, make sure the columns \"Snowparks\" and \"NightSki\" are boolean type in the end.\n",
    "Lastly, print the top five rows of the resulting dataframe.\n",
    "\n",
    "*Hint: You can use the pd.get_dummies() function from pandas for the first transformation*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4098a7f-2640-4577-9fe3-09e0bf7519b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code for 1. Transform the categorical column(s) you identified in a(iii) into one-hot encoding format. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "381d6bff-270d-4e5a-93e7-62dc97b0551e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code for 2. Transform the columns \"Snowparks\" and \"NightSki\" in `df` into boolean data type, where \"Yes\" should be `True` and \"No\" should be `False`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02638dbf-36cd-47d8-9eee-07591eeb5ed9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use df.dtypes to check if you correctly transform the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c3192a2-dedb-45ab-b5c0-86b49a7a15e9",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### **b(iii)** \n",
    "The previous analysis in b(i) showed that there are missing values in the 'SurfaceLifts' and 'GondolaLifts' columns.\\\n",
    "How would you impute these values? \\\n",
    "Motivate your approach and apply it to `df`.\n",
    "\n",
    "*Hint: Remember the semantics of the columns. Also, carefully assert your assumptions.*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0e34370-d15a-4f57-8df1-b11ae3e4b9a5",
   "metadata": {},
   "source": [
    "**Your Answer:** *(Motivate your approach.)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07628a31-b88b-4692-92ca-2aec93f36173",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a15869a-3c62-4363-9248-012b1ae373d0",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### **b(iv)**\n",
    "Impute the rest of the missing values using the knn-imputation method. To this end, apply the following steps:\n",
    "1. Create a working copy `df_tmp` of your updated `df`.\n",
    "2. For simplicity, drop the non-numerical columns (i.e., not of types integer or floats), which also inlcude the one-hot encoded and the boolean columns* you created earlier.\n",
    "3. Normalize the data in `df_tmp` (e.g., Standard score normalization). If the features have very different scales, knn can become very biased.\n",
    "4. Impute the missing values considering six neighbors.\n",
    "5. Invert the transformation applied upfront to enable more meaningful and intuitive visualizations.\n",
    "6. Append the columns you dropped in step 2.\n",
    " \n",
    "In the end, `df` should not contain missing values and have all the columns.\n",
    "\n",
    "\\*Note that by dropping the columns we lose the information of countries and the two boolean attributes (\"Snowparks\" and \"NightSki\") when imputing the missing values, which might be crucial for inferencing values such as the price for a ski pass. In practice, one should try to find if there are correlations before deciding whether to drop the columns or not.\n",
    "We drop the columns here to make the following steps easier because we only have to deal with numerical columns.\n",
    "\n",
    "*Hint: Be careful with the indices of your dataframes.*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1dc58ba-2699-4826-9dda-f0c5c135f32e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# your code for step 1 & 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7600aaaf-a864-47bb-aa62-76bfd644fd53",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# your code for step 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ce24019-3dba-438d-a7ea-dcc3244bbc53",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# your code for step 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42034302-c720-4f4d-9512-10bfa9842d70",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# your code for step 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6de8f607-4016-4a8c-966e-0bcd3ee086b9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# your code for step 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a5dd094-f496-4a98-81fb-1e941ee90472",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# assert df.isna().sum().sum() == 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52685085-a3d3-4a40-a979-069ec578c01e",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Question 2: Visualization (13 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08fdcbdb-792d-4c66-b5ef-4e86d6a955b5",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "In this task, you will analyze the data using different means of visualization.\n",
    "\n",
    "Start with the following preprocessed and integrated dataframe `df_v`. \\\n",
    "Note that it has a similar structure to the dataframe that you should obtain from the previous task, however, the values have been modified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "620155fc-f0c9-4f49-b778-6379c949bb2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "from pandas.plotting import scatter_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "570ecd50-427b-4203-bb2f-f2fcbd1052a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_v = pd.read_csv(\"./datasets/ski_resorts_visual.csv\", index_col=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9de753a8-98a9-4336-9e91-0acc1c73cf60",
   "metadata": {
    "tags": []
   },
   "source": [
    "### **a) (3pts)** \n",
    "To start the visual analysis, make a Scatter plot matrix to visually check if there are any correlations between the numerical attributes.\n",
    "\n",
    "*Hint: You can use the scatter_matrix from pandas.plotting or pairplot from seaborn to make the plot.*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12a7b99e-a13f-42b0-862b-91968efe7542",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d60f31f7-cba3-4c6d-be6f-fdefb5c6fb17",
   "metadata": {
    "tags": []
   },
   "source": [
    "### **b) (3pts)**\n",
    "Another way to detect correlation is to calculate the Pearson correlation coefficient. Calculate the correlation matrix for the numerical data and visualize the matrix using a heatmap. \n",
    "Briefly discuss your findings from the heatmap and the scatter plot you created in 2(a).\n",
    "\n",
    "Make sure to annotate the heatmap with the values of the correlation.\n",
    "\n",
    "*Hint: You can use the heatmap function from seaborn to make the plot.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bd3d2ce-7a30-4fa6-a5e9-46cefd79e26f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f387c1b8-4e34-4b68-88bf-774048366104",
   "metadata": {
    "tags": []
   },
   "source": [
    "### **c) (4pts)** \n",
    "We now want to have an overview of the attribute \"TotalSlope\" aggregated by different levels of hierarchy (Europe -> Country -> Resort). It seems that a tree map is suitable for this purpose.\n",
    "\n",
    "Make a tree map where\n",
    "- the root node represents Europe.\n",
    "- the child nodes of Europe are countries.\n",
    "- the child nodes of each country are the ski resorts.\n",
    "- the size of the rectangles is determined by the attribute \"TotalSlope\".\n",
    "\n",
    "Also, use the tree map to find out\n",
    "1. The sum of TotalSlopes of a country, list the top five countries and the corresponding values.\n",
    "2. The max value of TotalSlope of the five countries you identified in 1.\n",
    "\n",
    "*Hint: You can use the treemap function from plotly.express.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eed294b7-8822-45fe-adfc-4a4e4d024124",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6279cedc-ab47-4bc1-ac36-bdac4fb91f2b",
   "metadata": {},
   "source": [
    "**Your answer for...** \\\n",
    "*...  1. The sum of TotalSlopes of a country. List the top five countries and the corresponding values:* \\\n",
    "*...  2. The max value of TotalSlope of the five countries you identified in 1:*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cd91227-df95-4371-8538-fac931bc4745",
   "metadata": {
    "tags": []
   },
   "source": [
    "### **d) (3pts)** \n",
    "\n",
    "An alternative to a tree map is a sunburst plot, the principle is similar to a tree map. \n",
    "\n",
    "Recall from the lecture that:\n",
    "- Each ring is a different level of the hierarchy\n",
    "- Each segment of a ring belongs to one categorical value\n",
    "- The size of a segment is either divided proportionally to a value\n",
    "\n",
    "Now, we would like to have an overview of the attribute \"TotalLifts\" aggregated by different level of hierarchy.\n",
    "\n",
    "Make a sunburst plot where\n",
    "- the first hierarchy(ring) is \"Country\"\n",
    "- the second hierarchy(ring) is \"Snowparks\" (whether the resort has snowparks)\n",
    "- the third hierarchy(ring) is \"Resort\"\n",
    "- the size of the segments is determined by the attribute \"TotalLifts\".\n",
    "\n",
    "Then, briefly discuss your findings from the plot.\n",
    "\n",
    "*Hint: You can use the sunburst function from plotly.express.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7203fa15-8f8b-4cb7-b836-de047bb6a2bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e24702c9-93a5-47d9-88ef-cbd018ae840b",
   "metadata": {},
   "source": [
    "**Your answer:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62eb1982-1fa7-4829-9dfa-df0922f8de3c",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Question 3 - Frequent Item Sets and Association Rules (15 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cc7d780a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlxtend.preprocessing import TransactionEncoder\n",
    "from mlxtend.frequent_patterns import apriori\n",
    "from mlxtend.frequent_patterns import association_rules as arule"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2e7e462-5ead-4531-a306-7bf6047647d5",
   "metadata": {
    "tags": []
   },
   "source": [
    "### A real online retail transaction data set of two years.\n",
    "\n",
    "Data Set Information:\n",
    "This Retail dataset contains all the transactions occurring for a UK-based and registered, non-store online retail between 01/12/2009 and 09/12/2011.The company mainly sells unique all-occasion gift ware. Many customers of the company are wholesalers.\n",
    "\n",
    "Attribute Information:\n",
    "- Invoice: Invoice number. Nominal. A 6-digit integral number is uniquely assigned to each transaction. If the number starts with 'C' it refers to a canceled transaction.\n",
    "- StockCode: Product (item) code. Nominal. A 5-digit integral number uniquely assigned to each distinct product.\n",
    "- Description: Product (item) name. Nominal.\n",
    "- Quantity: The quantities of each product (item) per transaction. Numeric.\n",
    "- InvoiceDate: Invoice date and time. Numeric. The day and time when a transaction was generated.\n",
    "- Price: Unit price. Numeric. Product price per unit in sterling (£).\n",
    "- CustomerID: Customer number. Nominal. A 5-digit integral number is uniquely assigned to each customer. This number has postfix 'n'.\n",
    "- Country: Country name. Nominal. The name of the country where a customer resides."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9794fa4d-e5c8-4699-be1b-389dbd32b9c4",
   "metadata": {},
   "source": [
    "### a) Loading, exploring and preprocessing the dataset (4 pts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27a2d2b7-4dda-4a52-a597-80fdfaae628f",
   "metadata": {},
   "source": [
    "#### **a(i)** \n",
    "Load the data from `retail.csv` and save it under the variable `retail_df`. Display the first few lines of the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bc75ad8d-88bb-4ef3-ab32-ae5a1f38954e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Invoice</th>\n",
       "      <th>StockCode</th>\n",
       "      <th>Description</th>\n",
       "      <th>Quantity</th>\n",
       "      <th>InvoiceDate</th>\n",
       "      <th>Price</th>\n",
       "      <th>CustomerID</th>\n",
       "      <th>Country</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>536365</td>\n",
       "      <td>85123A</td>\n",
       "      <td>WHITE HANGING HEART T-LIGHT HOLDER</td>\n",
       "      <td>6</td>\n",
       "      <td>01/12/2010 08:26</td>\n",
       "      <td>2.55</td>\n",
       "      <td>178500</td>\n",
       "      <td>United Kingdom</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>536365</td>\n",
       "      <td>71053</td>\n",
       "      <td>WHITE METAL LANTERN</td>\n",
       "      <td>6</td>\n",
       "      <td>01/12/2010 08:26</td>\n",
       "      <td>3.39</td>\n",
       "      <td>178500</td>\n",
       "      <td>United Kingdom</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>536365</td>\n",
       "      <td>84406B</td>\n",
       "      <td>CREAM CUPID HEARTS COAT HANGER</td>\n",
       "      <td>8</td>\n",
       "      <td>01/12/2010 08:26</td>\n",
       "      <td>2.75</td>\n",
       "      <td>178500</td>\n",
       "      <td>United Kingdom</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>536365</td>\n",
       "      <td>84029G</td>\n",
       "      <td>KNITTED UNION FLAG HOT WATER BOTTLE</td>\n",
       "      <td>6</td>\n",
       "      <td>01/12/2010 08:26</td>\n",
       "      <td>3.39</td>\n",
       "      <td>178500</td>\n",
       "      <td>United Kingdom</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>536365</td>\n",
       "      <td>84029E</td>\n",
       "      <td>RED WOOLLY HOTTIE WHITE HEART.</td>\n",
       "      <td>6</td>\n",
       "      <td>01/12/2010 08:26</td>\n",
       "      <td>3.39</td>\n",
       "      <td>178500</td>\n",
       "      <td>United Kingdom</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Invoice StockCode                          Description  Quantity  \\\n",
       "0  536365    85123A   WHITE HANGING HEART T-LIGHT HOLDER         6   \n",
       "1  536365     71053                  WHITE METAL LANTERN         6   \n",
       "2  536365    84406B       CREAM CUPID HEARTS COAT HANGER         8   \n",
       "3  536365    84029G  KNITTED UNION FLAG HOT WATER BOTTLE         6   \n",
       "4  536365    84029E       RED WOOLLY HOTTIE WHITE HEART.         6   \n",
       "\n",
       "        InvoiceDate  Price  CustomerID         Country  \n",
       "0  01/12/2010 08:26   2.55      178500  United Kingdom  \n",
       "1  01/12/2010 08:26   3.39      178500  United Kingdom  \n",
       "2  01/12/2010 08:26   2.75      178500  United Kingdom  \n",
       "3  01/12/2010 08:26   3.39      178500  United Kingdom  \n",
       "4  01/12/2010 08:26   3.39      178500  United Kingdom  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# your code\n",
    "import pandas as pd\n",
    "\n",
    "retail_df = pd.read_csv('retail.csv')\n",
    "\n",
    "retail_df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eba06313-e9f1-43cd-ab08-07e56458234b",
   "metadata": {},
   "source": [
    "#### **a(ii)** \n",
    "To get to know the dataset, do the following:\n",
    "\n",
    "- Show the number of rows in the dataset.\n",
    "- Show the number of unique customers.\n",
    "- Show the number of unique product names.\n",
    "- Show the number of unique invoices.\n",
    "- Show the number and the list of all the countries where the customers reside."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eadfbf62-1e6b-4b5d-9556-46d9c2fc2614",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows : 406525\n",
      "Unique Customers: 4367\n",
      "Unique Product Names: 3896\n",
      "Unique Invoices: 22177\n",
      "\n",
      "Unique Countries: 35\n",
      "Unique Countries List: ['United Kingdom' 'France' 'Australia' 'Netherlands' 'Germany' 'Norway'\n",
      " 'EIRE' 'Switzerland' 'Spain' 'Poland' 'Portugal' 'Italy' 'Belgium'\n",
      " 'Lithuania' 'Japan' 'Iceland' 'Channel Islands' 'Denmark' 'Cyprus'\n",
      " 'Sweden' 'Austria' 'Israel' 'Finland' 'Greece' 'Singapore' 'Lebanon'\n",
      " 'United Arab Emirates' 'Saudi Arabia' 'Czech Republic' 'Canada' 'Brazil'\n",
      " 'USA' 'Bahrain' 'Malta' 'RSA']\n"
     ]
    }
   ],
   "source": [
    "# your code\n",
    "print(f\"Number of rows : {retail_df.shape[0]}\")\n",
    "print(f\"Unique Customers: {len(retail_df.CustomerID.unique())}\")\n",
    "print(f\"Unique Product Names: {len(retail_df.Description.unique())}\")\n",
    "print(f\"Unique Invoices: {len(retail_df.Invoice.unique())}\")\n",
    "print()\n",
    "unique_countries = retail_df.Country.unique()\n",
    "print(f\"Unique Countries: {len(unique_countries)}\")\n",
    "print(f\"Unique Countries List: {unique_countries}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "071fc555-19fd-4488-8e2c-efdb0cb785be",
   "metadata": {},
   "source": [
    "#### **a(iii)** \n",
    "You are interested in analyzing itemsets that are frequently purchased together. Before continuing with that task, you have to make sure that the data are fit for such analysis. 1) More precisely, you want to make sure that there are no missing values in the data. 2) Moreover, you want to ensure that each item's name in the \"Description\" is consistent. E.g., you want \"Description\" values such as \" coffee black\", \"coffee &nbsp;black\", \" coffee black &nbsp;\", etc. to be mapped to the same value (e.g. \"coffee black\"). 3) Last but not least, you want to remove transactions that were canceled. Such transactions correspond to rows where the invoice number starts with letter 'C'.\n",
    "\n",
    "Apply these preprocessing steps to the dataset `retail_df` and apply them on the dataframe itself (e.g. set inplace=True)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7bb891d0-6362-47c5-903d-4caf7333a2c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:8: DeprecationWarning: invalid escape sequence \\s\n",
      "<>:8: DeprecationWarning: invalid escape sequence \\s\n",
      "C:\\Users\\DELL\\AppData\\Local\\Temp\\ipykernel_2800\\2092951492.py:8: DeprecationWarning: invalid escape sequence \\s\n",
      "  retail_df['Description'] = retail_df['Description'].str.replace('\\s+', ' ') # remove multiple whitespaces\n",
      "C:\\Users\\DELL\\AppData\\Local\\Temp\\ipykernel_2800\\2092951492.py:8: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  retail_df['Description'] = retail_df['Description'].str.replace('\\s+', ' ') # remove multiple whitespaces\n"
     ]
    }
   ],
   "source": [
    "# your code\n",
    "\n",
    "# Missing Values\n",
    "retail_df.dropna(inplace=True)\n",
    "\n",
    "# Consistency in descriptions\n",
    "retail_df['Description'] = retail_df['Description'].str.strip() # remove leading and trailing whitespaces\n",
    "retail_df['Description'] = retail_df['Description'].str.replace('\\s+', ' ') # remove multiple whitespaces\n",
    "\n",
    "# Remove Cancelled Transactions\n",
    "retail_df.drop(retail_df[retail_df.Invoice.str.startswith('C')].index, inplace=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cdb88ce-11d8-405f-94b8-a0d670da853a",
   "metadata": {},
   "source": [
    "#### **a(iv)** \n",
    "After applying the preprocessing steps in **a(iii)** , repeat again the task **a(ii)**, that is:\n",
    "\n",
    "- Show the number of rows in the dataset.\n",
    "- Show the number of unique customers.\n",
    "- Show the number of all unique product names.\n",
    "- Show the list of all the countries where the customers reside.\n",
    "\n",
    "Which values changed?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1ff1aab7-1048-4043-989b-cd97437f0ed8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows : 397621\n",
      "Unique Customers: 4334\n",
      "Unique Product Names: 3858\n",
      "Unique Invoices: 18524\n",
      "\n",
      "Unique Countries: 35\n",
      "Unique Countries List: ['United Kingdom' 'France' 'Australia' 'Netherlands' 'Germany' 'Norway'\n",
      " 'EIRE' 'Switzerland' 'Spain' 'Poland' 'Portugal' 'Italy' 'Belgium'\n",
      " 'Lithuania' 'Japan' 'Iceland' 'Channel Islands' 'Denmark' 'Cyprus'\n",
      " 'Sweden' 'Finland' 'Austria' 'Greece' 'Singapore' 'Lebanon'\n",
      " 'United Arab Emirates' 'Israel' 'Saudi Arabia' 'Czech Republic' 'Canada'\n",
      " 'Brazil' 'USA' 'Bahrain' 'Malta' 'RSA']\n"
     ]
    }
   ],
   "source": [
    "# your code\n",
    "print(f\"Number of rows : {retail_df.shape[0]}\")\n",
    "print(f\"Unique Customers: {len(retail_df.CustomerID.unique())}\")\n",
    "print(f\"Unique Product Names: {len(retail_df.Description.unique())}\")\n",
    "print(f\"Unique Invoices: {len(retail_df.Invoice.unique())}\")\n",
    "print()\n",
    "unique_countries = retail_df.Country.unique()\n",
    "print(f\"Unique Countries: {len(unique_countries)}\")\n",
    "print(f\"Unique Countries List: {unique_countries}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "51ace216-572c-47d9-977f-52c62d2cb26e",
   "metadata": {},
   "source": [
    "**Your answer:** \n",
    "\n",
    "Property | Before | After |\n",
    "|--------------|-----------|------------|\n",
    "Number of rows | 406525 | 397621 |\n",
    "Unique Customers | 4367 | 4334 |\n",
    "Unique Product Names | 3896 | 3858 |\n",
    "Unique Invoices | 22177 | 18524 |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "105a9af7-a3c1-4d19-8cd8-778276106c52",
   "metadata": {},
   "source": [
    "### b) Frequent itemsets and Association rules (8 pts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d55a65f-a5a3-4616-8365-d6a99f0b4386",
   "metadata": {},
   "source": [
    "#### **b(i)** \n",
    "Each invoice number in the dataset identifies a unique transaction. There are potentially many rows in the dataframe having the same invoice number. We want to analyze items that are frequently purchased together, that is, items that appear in the same transaction.\n",
    "\n",
    "Create a new dataframe named `transaction_df` with two columns: \"Invoice\" and \"Description\". Here the \"Invoice\" value is the index of the dataframe (the unique number identifying each row) and \"Description\" is the column containing all items (without duplicates) purchased within the transaction with that invoice number. Display the first few rows of your dataframe. How many rows does the `transaction_df` have?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ec046573-e55d-4ea4-a9c6-7f9c82d8d5cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Invoice</th>\n",
       "      <th>Description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>536365</td>\n",
       "      <td>[WHITE HANGING HEART T-LIGHT HOLDER, WHITE MET...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>536366</td>\n",
       "      <td>[HAND WARMER UNION JACK, HAND WARMER RED POLKA...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>536367</td>\n",
       "      <td>[ASSORTED COLOUR BIRD ORNAMENT, POPPY'S PLAYHO...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>536368</td>\n",
       "      <td>[JAM MAKING SET WITH JARS, RED COAT RACK PARIS...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>536369</td>\n",
       "      <td>[BATH BUILDING BLOCK WORD]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>536370</td>\n",
       "      <td>[ALARM CLOCK BAKELIKE PINK, ALARM CLOCK BAKELI...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>536371</td>\n",
       "      <td>[PAPER CHAIN KIT 50'S CHRISTMAS]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>536372</td>\n",
       "      <td>[HAND WARMER RED POLKA DOT, HAND WARMER UNION ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>536373</td>\n",
       "      <td>[WHITE HANGING HEART T-LIGHT HOLDER, WHITE MET...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>536374</td>\n",
       "      <td>[VICTORIAN SEWING BOX LARGE]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Invoice                                        Description\n",
       "0  536365  [WHITE HANGING HEART T-LIGHT HOLDER, WHITE MET...\n",
       "1  536366  [HAND WARMER UNION JACK, HAND WARMER RED POLKA...\n",
       "2  536367  [ASSORTED COLOUR BIRD ORNAMENT, POPPY'S PLAYHO...\n",
       "3  536368  [JAM MAKING SET WITH JARS, RED COAT RACK PARIS...\n",
       "4  536369                         [BATH BUILDING BLOCK WORD]\n",
       "5  536370  [ALARM CLOCK BAKELIKE PINK, ALARM CLOCK BAKELI...\n",
       "6  536371                   [PAPER CHAIN KIT 50'S CHRISTMAS]\n",
       "7  536372  [HAND WARMER RED POLKA DOT, HAND WARMER UNION ...\n",
       "8  536373  [WHITE HANGING HEART T-LIGHT HOLDER, WHITE MET...\n",
       "9  536374                       [VICTORIAN SEWING BOX LARGE]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# your code\n",
    "\n",
    "transaction_df = retail_df.groupby('Invoice')['Description'].apply(lambda x: x.unique())\n",
    "\n",
    "transaction_df = transaction_df.reset_index()\n",
    "\n",
    "transaction_df.columns = ['Invoice', 'Description']\n",
    "\n",
    "transaction_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e9a1a3d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows: 18524\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of rows: {transaction_df.shape[0]}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "97f7615d-f7f2-4881-b081-23505c0875d6",
   "metadata": {},
   "source": [
    "**Your answer:** \n",
    "18524"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8abcf0c1-0445-4701-874f-d7f8f600075f",
   "metadata": {},
   "source": [
    "#### **b(ii)** \n",
    "Next, we want to compute frequent itemsets and association rules based on the sets of items ordered together. Use the TransactionEncoder to transform `transaction_df` into a matrix such that the value in the i-th row and the j-th column is $True$ if the i-th itemset contains product j, and $False$ otherwise. Save the matrix into a dataframe named `transactions`. Display the shape of the matrix.\n",
    "\n",
    "*Hint: Note that your dataframe 'transactions' must contain as many rows as there are invoice numbers and as many columns as there are unique products.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c5c0b0e0-46c9-4b8d-873a-1d0d1136ebdf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(18524, 3858)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>10 COLOUR SPACEBOY PEN</th>\n",
       "      <th>12 COLOURED PARTY BALLOONS</th>\n",
       "      <th>12 DAISY PEGS IN WOOD BOX</th>\n",
       "      <th>12 EGG HOUSE PAINTED WOOD</th>\n",
       "      <th>12 HANGING EGGS HAND PAINTED</th>\n",
       "      <th>12 IVORY ROSE PEG PLACE SETTINGS</th>\n",
       "      <th>12 MESSAGE CARDS WITH ENVELOPES</th>\n",
       "      <th>12 PENCIL SMALL TUBE WOODLAND</th>\n",
       "      <th>12 PENCILS SMALL TUBE RED RETROSPOT</th>\n",
       "      <th>12 PENCILS SMALL TUBE SKULL</th>\n",
       "      <th>...</th>\n",
       "      <th>ZINC STAR T-LIGHT HOLDER</th>\n",
       "      <th>ZINC SWEETHEART SOAP DISH</th>\n",
       "      <th>ZINC SWEETHEART WIRE LETTER RACK</th>\n",
       "      <th>ZINC T-LIGHT HOLDER STAR LARGE</th>\n",
       "      <th>ZINC T-LIGHT HOLDER STARS LARGE</th>\n",
       "      <th>ZINC T-LIGHT HOLDER STARS SMALL</th>\n",
       "      <th>ZINC TOP 2 DOOR WOODEN SHELF</th>\n",
       "      <th>ZINC WILLIE WINKIE CANDLE STICK</th>\n",
       "      <th>ZINC WIRE KITCHEN ORGANISER</th>\n",
       "      <th>ZINC WIRE SWEETHEART LETTER TRAY</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 3858 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   10 COLOUR SPACEBOY PEN  12 COLOURED PARTY BALLOONS  \\\n",
       "0                   False                       False   \n",
       "1                   False                       False   \n",
       "2                   False                       False   \n",
       "3                   False                       False   \n",
       "4                   False                       False   \n",
       "\n",
       "   12 DAISY PEGS IN WOOD BOX  12 EGG HOUSE PAINTED WOOD  \\\n",
       "0                      False                      False   \n",
       "1                      False                      False   \n",
       "2                      False                      False   \n",
       "3                      False                      False   \n",
       "4                      False                      False   \n",
       "\n",
       "   12 HANGING EGGS HAND PAINTED  12 IVORY ROSE PEG PLACE SETTINGS  \\\n",
       "0                         False                             False   \n",
       "1                         False                             False   \n",
       "2                         False                             False   \n",
       "3                         False                             False   \n",
       "4                         False                             False   \n",
       "\n",
       "   12 MESSAGE CARDS WITH ENVELOPES  12 PENCIL SMALL TUBE WOODLAND  \\\n",
       "0                            False                          False   \n",
       "1                            False                          False   \n",
       "2                            False                          False   \n",
       "3                            False                          False   \n",
       "4                            False                          False   \n",
       "\n",
       "   12 PENCILS SMALL TUBE RED RETROSPOT  12 PENCILS SMALL TUBE SKULL  ...  \\\n",
       "0                                False                        False  ...   \n",
       "1                                False                        False  ...   \n",
       "2                                False                        False  ...   \n",
       "3                                False                        False  ...   \n",
       "4                                False                        False  ...   \n",
       "\n",
       "   ZINC STAR T-LIGHT HOLDER  ZINC SWEETHEART SOAP DISH  \\\n",
       "0                     False                      False   \n",
       "1                     False                      False   \n",
       "2                     False                      False   \n",
       "3                     False                      False   \n",
       "4                     False                      False   \n",
       "\n",
       "   ZINC SWEETHEART WIRE LETTER RACK  ZINC T-LIGHT HOLDER STAR LARGE  \\\n",
       "0                             False                           False   \n",
       "1                             False                           False   \n",
       "2                             False                           False   \n",
       "3                             False                           False   \n",
       "4                             False                           False   \n",
       "\n",
       "   ZINC T-LIGHT HOLDER STARS LARGE  ZINC T-LIGHT HOLDER STARS SMALL  \\\n",
       "0                            False                            False   \n",
       "1                            False                            False   \n",
       "2                            False                            False   \n",
       "3                            False                            False   \n",
       "4                            False                            False   \n",
       "\n",
       "   ZINC TOP 2 DOOR WOODEN SHELF  ZINC WILLIE WINKIE CANDLE STICK  \\\n",
       "0                         False                            False   \n",
       "1                         False                            False   \n",
       "2                         False                            False   \n",
       "3                         False                            False   \n",
       "4                         False                            False   \n",
       "\n",
       "   ZINC WIRE KITCHEN ORGANISER  ZINC WIRE SWEETHEART LETTER TRAY  \n",
       "0                        False                             False  \n",
       "1                        False                             False  \n",
       "2                        False                             False  \n",
       "3                        False                             False  \n",
       "4                        False                             False  \n",
       "\n",
       "[5 rows x 3858 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# your code\n",
    "te = TransactionEncoder()\n",
    "te_ary = te.fit(transaction_df.Description).transform(transaction_df.Description)\n",
    "transactions = pd.DataFrame(te_ary, columns=te.columns_)\n",
    "\n",
    "print(transactions.shape)\n",
    "transactions.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a718591-452e-4d69-be6f-f2e53e2060c7",
   "metadata": {},
   "source": [
    "#### **b(iii)** \n",
    "Use the apriori method on `transactions` to obtain all frequent itemsets using min_support=0.01. Display all frequent itemsets that have at least three items. What support count does an itemset have for our case if it satisfies min_support=0.01?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1463627c-e2c3-43b0-b8c7-f1ad0591f9a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>support</th>\n",
       "      <th>itemsets</th>\n",
       "      <th>length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.013064</td>\n",
       "      <td>(10 COLOUR SPACEBOY PEN)</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.010041</td>\n",
       "      <td>(12 MESSAGE CARDS WITH ENVELOPES)</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.014900</td>\n",
       "      <td>(12 PENCIL SMALL TUBE WOODLAND)</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.016573</td>\n",
       "      <td>(12 PENCILS SMALL TUBE RED RETROSPOT)</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.015709</td>\n",
       "      <td>(12 PENCILS SMALL TUBE SKULL)</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>977</th>\n",
       "      <td>0.011769</td>\n",
       "      <td>(LUNCH BAG SUKI DESIGN, LUNCH BAG SPACEBOY DES...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>978</th>\n",
       "      <td>0.014306</td>\n",
       "      <td>(PINK REGENCY TEACUP AND SAUCER, ROSES REGENCY...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>979</th>\n",
       "      <td>0.010041</td>\n",
       "      <td>(POPPY'S PLAYHOUSE LIVINGROOM, POPPY'S PLAYHOU...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>980</th>\n",
       "      <td>0.012902</td>\n",
       "      <td>(GREEN REGENCY TEACUP AND SAUCER, PINK REGENCY...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>981</th>\n",
       "      <td>0.010419</td>\n",
       "      <td>(LUNCH BAG RED RETROSPOT, LUNCH BAG PINK POLKA...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>982 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      support                                           itemsets  length\n",
       "0    0.013064                           (10 COLOUR SPACEBOY PEN)       1\n",
       "1    0.010041                  (12 MESSAGE CARDS WITH ENVELOPES)       1\n",
       "2    0.014900                    (12 PENCIL SMALL TUBE WOODLAND)       1\n",
       "3    0.016573              (12 PENCILS SMALL TUBE RED RETROSPOT)       1\n",
       "4    0.015709                      (12 PENCILS SMALL TUBE SKULL)       1\n",
       "..        ...                                                ...     ...\n",
       "977  0.011769  (LUNCH BAG SUKI DESIGN, LUNCH BAG SPACEBOY DES...       3\n",
       "978  0.014306  (PINK REGENCY TEACUP AND SAUCER, ROSES REGENCY...       3\n",
       "979  0.010041  (POPPY'S PLAYHOUSE LIVINGROOM, POPPY'S PLAYHOU...       3\n",
       "980  0.012902  (GREEN REGENCY TEACUP AND SAUCER, PINK REGENCY...       4\n",
       "981  0.010419  (LUNCH BAG RED RETROSPOT, LUNCH BAG PINK POLKA...       4\n",
       "\n",
       "[982 rows x 3 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# your code\n",
    "\n",
    "# apriori(transactions, min_support=0.01, use_colnames=True)\n",
    "frequent_itemsets = apriori(transactions, min_support=0.01, use_colnames=True)\n",
    "frequent_itemsets['length'] = frequent_itemsets['itemsets'].apply(lambda x: len(x))\n",
    "frequent_itemsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a84318f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>support</th>\n",
       "      <th>itemsets</th>\n",
       "      <th>length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>923</th>\n",
       "      <td>0.011607</td>\n",
       "      <td>(ALARM CLOCK BAKELIKE GREEN, ALARM CLOCK BAKEL...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>924</th>\n",
       "      <td>0.014414</td>\n",
       "      <td>(ALARM CLOCK BAKELIKE GREEN, ALARM CLOCK BAKEL...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>925</th>\n",
       "      <td>0.014630</td>\n",
       "      <td>(GREEN REGENCY TEACUP AND SAUCER, PINK REGENCY...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>926</th>\n",
       "      <td>0.021054</td>\n",
       "      <td>(GREEN REGENCY TEACUP AND SAUCER, PINK REGENCY...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>927</th>\n",
       "      <td>0.016843</td>\n",
       "      <td>(GREEN REGENCY TEACUP AND SAUCER, ROSES REGENC...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      support                                           itemsets  length\n",
       "923  0.011607  (ALARM CLOCK BAKELIKE GREEN, ALARM CLOCK BAKEL...       3\n",
       "924  0.014414  (ALARM CLOCK BAKELIKE GREEN, ALARM CLOCK BAKEL...       3\n",
       "925  0.014630  (GREEN REGENCY TEACUP AND SAUCER, PINK REGENCY...       3\n",
       "926  0.021054  (GREEN REGENCY TEACUP AND SAUCER, PINK REGENCY...       3\n",
       "927  0.016843  (GREEN REGENCY TEACUP AND SAUCER, ROSES REGENC...       3"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "frequent_itemsets[(frequent_itemsets['length'] >= 3)].head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2bcc5fab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>itemset</th>\n",
       "      <th>support_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>(10 COLOUR SPACEBOY PEN)</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>(RED DINER WALL CLOCK, BLUE DINER WALL CLOCK)</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>(BLUE HARMONICA IN BOX, RED HARMONICA IN BOX)</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>(BLUE STRIPE CERAMIC DRAWER KNOB, RED STRIPE C...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>(CANDLEHOLDER PINK HANGING HEART, WHITE HANGIN...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>977</th>\n",
       "      <td>(PAPER CHAIN KIT 50'S CHRISTMAS)</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>978</th>\n",
       "      <td>(PAPER CHAIN KIT EMPIRE)</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>979</th>\n",
       "      <td>(PAPER CHAIN KIT RETROSPOT)</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>980</th>\n",
       "      <td>(PAPER CHAIN KIT VINTAGE CHRISTMAS)</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>981</th>\n",
       "      <td>(LUNCH BAG RED RETROSPOT, LUNCH BAG PINK POLKA...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>982 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               itemset  support_count\n",
       "0                             (10 COLOUR SPACEBOY PEN)              1\n",
       "1        (RED DINER WALL CLOCK, BLUE DINER WALL CLOCK)              1\n",
       "2        (BLUE HARMONICA IN BOX, RED HARMONICA IN BOX)              1\n",
       "3    (BLUE STRIPE CERAMIC DRAWER KNOB, RED STRIPE C...              1\n",
       "4    (CANDLEHOLDER PINK HANGING HEART, WHITE HANGIN...              1\n",
       "..                                                 ...            ...\n",
       "977                   (PAPER CHAIN KIT 50'S CHRISTMAS)              1\n",
       "978                           (PAPER CHAIN KIT EMPIRE)              1\n",
       "979                        (PAPER CHAIN KIT RETROSPOT)              1\n",
       "980                (PAPER CHAIN KIT VINTAGE CHRISTMAS)              1\n",
       "981  (LUNCH BAG RED RETROSPOT, LUNCH BAG PINK POLKA...              1\n",
       "\n",
       "[982 rows x 2 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "itemsets = frequent_itemsets['itemsets']\n",
    "\n",
    "itemsets_counts = itemsets.value_counts()\n",
    "\n",
    "# Create a new dataframe from the surname_counts Series\n",
    "itemsets_support_counts = pd.DataFrame({'itemset': itemsets_counts.index, 'support_count': itemsets_counts.values})\n",
    "\n",
    "itemsets_support_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dbd0b5ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(itemsets_support_counts['support_count'].to_list())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f0ad89a8-70d2-4734-9780-c964e14fbf23",
   "metadata": {},
   "source": [
    "**Your answer:**\n",
    "1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14eb37dd-da50-4b68-988d-23500611955b",
   "metadata": {},
   "source": [
    "#### **b(iv)**\n",
    "Now we will discover association rules from the frequent itemsets. Using only the frequent itemsets with min_support=0.01 (the ones obtained in **b(iii)**), generate different association rules using min_conf=0.6 and min_conf=0.9 as thresholds. Show the association rules for each of the thresholds. What do you notice w.r.t. the number of association rules produced for the different thresholds?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "aba91aa5-f2ef-4e68-a70c-6f112c3b9ec6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>antecedents</th>\n",
       "      <th>consequents</th>\n",
       "      <th>antecedent support</th>\n",
       "      <th>consequent support</th>\n",
       "      <th>support</th>\n",
       "      <th>confidence</th>\n",
       "      <th>lift</th>\n",
       "      <th>leverage</th>\n",
       "      <th>conviction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>(ALARM CLOCK BAKELIKE CHOCOLATE)</td>\n",
       "      <td>(ALARM CLOCK BAKELIKE GREEN)</td>\n",
       "      <td>0.017383</td>\n",
       "      <td>0.042593</td>\n",
       "      <td>0.011337</td>\n",
       "      <td>0.652174</td>\n",
       "      <td>15.311622</td>\n",
       "      <td>0.010596</td>\n",
       "      <td>2.752544</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>(ALARM CLOCK BAKELIKE CHOCOLATE)</td>\n",
       "      <td>(ALARM CLOCK BAKELIKE RED)</td>\n",
       "      <td>0.017383</td>\n",
       "      <td>0.047290</td>\n",
       "      <td>0.012200</td>\n",
       "      <td>0.701863</td>\n",
       "      <td>14.841686</td>\n",
       "      <td>0.011378</td>\n",
       "      <td>3.195548</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>(ALARM CLOCK BAKELIKE ORANGE)</td>\n",
       "      <td>(ALARM CLOCK BAKELIKE GREEN)</td>\n",
       "      <td>0.019380</td>\n",
       "      <td>0.042593</td>\n",
       "      <td>0.011876</td>\n",
       "      <td>0.612813</td>\n",
       "      <td>14.387522</td>\n",
       "      <td>0.011051</td>\n",
       "      <td>2.472726</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>(ALARM CLOCK BAKELIKE GREEN)</td>\n",
       "      <td>(ALARM CLOCK BAKELIKE RED)</td>\n",
       "      <td>0.042593</td>\n",
       "      <td>0.047290</td>\n",
       "      <td>0.028612</td>\n",
       "      <td>0.671736</td>\n",
       "      <td>14.204617</td>\n",
       "      <td>0.026597</td>\n",
       "      <td>2.902271</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>(ALARM CLOCK BAKELIKE RED)</td>\n",
       "      <td>(ALARM CLOCK BAKELIKE GREEN)</td>\n",
       "      <td>0.047290</td>\n",
       "      <td>0.042593</td>\n",
       "      <td>0.028612</td>\n",
       "      <td>0.605023</td>\n",
       "      <td>14.204617</td>\n",
       "      <td>0.026597</td>\n",
       "      <td>2.423954</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>(PINK REGENCY TEACUP AND SAUCER, REGENCY CAKES...</td>\n",
       "      <td>(GREEN REGENCY TEACUP AND SAUCER, ROSES REGENC...</td>\n",
       "      <td>0.016681</td>\n",
       "      <td>0.029205</td>\n",
       "      <td>0.012902</td>\n",
       "      <td>0.773463</td>\n",
       "      <td>26.483594</td>\n",
       "      <td>0.012415</td>\n",
       "      <td>4.285365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>(LUNCH BAG RED RETROSPOT, LUNCH BAG PINK POLKA...</td>\n",
       "      <td>(LUNCH BAG CARS BLUE)</td>\n",
       "      <td>0.016627</td>\n",
       "      <td>0.052149</td>\n",
       "      <td>0.010419</td>\n",
       "      <td>0.626623</td>\n",
       "      <td>12.016119</td>\n",
       "      <td>0.009552</td>\n",
       "      <td>2.538593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>(LUNCH BAG RED RETROSPOT, LUNCH BAG PINK POLKA...</td>\n",
       "      <td>(LUNCH BAG BLACK SKULL.)</td>\n",
       "      <td>0.015062</td>\n",
       "      <td>0.056791</td>\n",
       "      <td>0.010419</td>\n",
       "      <td>0.691756</td>\n",
       "      <td>12.180697</td>\n",
       "      <td>0.009564</td>\n",
       "      <td>3.059945</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>(LUNCH BAG RED RETROSPOT, LUNCH BAG BLACK SKUL...</td>\n",
       "      <td>(LUNCH BAG PINK POLKADOT)</td>\n",
       "      <td>0.014144</td>\n",
       "      <td>0.050259</td>\n",
       "      <td>0.010419</td>\n",
       "      <td>0.736641</td>\n",
       "      <td>14.656866</td>\n",
       "      <td>0.009708</td>\n",
       "      <td>3.606262</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>(LUNCH BAG PINK POLKADOT, LUNCH BAG BLACK SKUL...</td>\n",
       "      <td>(LUNCH BAG RED RETROSPOT)</td>\n",
       "      <td>0.014036</td>\n",
       "      <td>0.069531</td>\n",
       "      <td>0.010419</td>\n",
       "      <td>0.742308</td>\n",
       "      <td>10.675860</td>\n",
       "      <td>0.009443</td>\n",
       "      <td>3.610774</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>150 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           antecedents  \\\n",
       "0                     (ALARM CLOCK BAKELIKE CHOCOLATE)   \n",
       "1                     (ALARM CLOCK BAKELIKE CHOCOLATE)   \n",
       "2                        (ALARM CLOCK BAKELIKE ORANGE)   \n",
       "3                         (ALARM CLOCK BAKELIKE GREEN)   \n",
       "4                           (ALARM CLOCK BAKELIKE RED)   \n",
       "..                                                 ...   \n",
       "145  (PINK REGENCY TEACUP AND SAUCER, REGENCY CAKES...   \n",
       "146  (LUNCH BAG RED RETROSPOT, LUNCH BAG PINK POLKA...   \n",
       "147  (LUNCH BAG RED RETROSPOT, LUNCH BAG PINK POLKA...   \n",
       "148  (LUNCH BAG RED RETROSPOT, LUNCH BAG BLACK SKUL...   \n",
       "149  (LUNCH BAG PINK POLKADOT, LUNCH BAG BLACK SKUL...   \n",
       "\n",
       "                                           consequents  antecedent support  \\\n",
       "0                         (ALARM CLOCK BAKELIKE GREEN)            0.017383   \n",
       "1                           (ALARM CLOCK BAKELIKE RED)            0.017383   \n",
       "2                         (ALARM CLOCK BAKELIKE GREEN)            0.019380   \n",
       "3                           (ALARM CLOCK BAKELIKE RED)            0.042593   \n",
       "4                         (ALARM CLOCK BAKELIKE GREEN)            0.047290   \n",
       "..                                                 ...                 ...   \n",
       "145  (GREEN REGENCY TEACUP AND SAUCER, ROSES REGENC...            0.016681   \n",
       "146                              (LUNCH BAG CARS BLUE)            0.016627   \n",
       "147                           (LUNCH BAG BLACK SKULL.)            0.015062   \n",
       "148                          (LUNCH BAG PINK POLKADOT)            0.014144   \n",
       "149                          (LUNCH BAG RED RETROSPOT)            0.014036   \n",
       "\n",
       "     consequent support   support  confidence       lift  leverage  conviction  \n",
       "0              0.042593  0.011337    0.652174  15.311622  0.010596    2.752544  \n",
       "1              0.047290  0.012200    0.701863  14.841686  0.011378    3.195548  \n",
       "2              0.042593  0.011876    0.612813  14.387522  0.011051    2.472726  \n",
       "3              0.047290  0.028612    0.671736  14.204617  0.026597    2.902271  \n",
       "4              0.042593  0.028612    0.605023  14.204617  0.026597    2.423954  \n",
       "..                  ...       ...         ...        ...       ...         ...  \n",
       "145            0.029205  0.012902    0.773463  26.483594  0.012415    4.285365  \n",
       "146            0.052149  0.010419    0.626623  12.016119  0.009552    2.538593  \n",
       "147            0.056791  0.010419    0.691756  12.180697  0.009564    3.059945  \n",
       "148            0.050259  0.010419    0.736641  14.656866  0.009708    3.606262  \n",
       "149            0.069531  0.010419    0.742308  10.675860  0.009443    3.610774  \n",
       "\n",
       "[150 rows x 9 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# your code\n",
    "\n",
    "rule_conf_p6 = arule(frequent_itemsets, metric=\"confidence\", min_threshold=0.6)\n",
    "rule_conf_p6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b60f6fc8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>antecedents</th>\n",
       "      <th>consequents</th>\n",
       "      <th>antecedent support</th>\n",
       "      <th>consequent support</th>\n",
       "      <th>support</th>\n",
       "      <th>confidence</th>\n",
       "      <th>lift</th>\n",
       "      <th>leverage</th>\n",
       "      <th>conviction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>(REGENCY TEA PLATE PINK)</td>\n",
       "      <td>(REGENCY TEA PLATE GREEN)</td>\n",
       "      <td>0.012092</td>\n",
       "      <td>0.014576</td>\n",
       "      <td>0.010905</td>\n",
       "      <td>0.901786</td>\n",
       "      <td>61.869180</td>\n",
       "      <td>0.010729</td>\n",
       "      <td>10.033411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>(POPPY'S PLAYHOUSE LIVINGROOM, POPPY'S PLAYHOU...</td>\n",
       "      <td>(POPPY'S PLAYHOUSE KITCHEN)</td>\n",
       "      <td>0.011067</td>\n",
       "      <td>0.018678</td>\n",
       "      <td>0.010041</td>\n",
       "      <td>0.907317</td>\n",
       "      <td>48.575553</td>\n",
       "      <td>0.009834</td>\n",
       "      <td>10.587943</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>(PINK REGENCY TEACUP AND SAUCER, ROSES REGENCY...</td>\n",
       "      <td>(GREEN REGENCY TEACUP AND SAUCER)</td>\n",
       "      <td>0.014306</td>\n",
       "      <td>0.037303</td>\n",
       "      <td>0.012902</td>\n",
       "      <td>0.901887</td>\n",
       "      <td>24.177353</td>\n",
       "      <td>0.012369</td>\n",
       "      <td>9.812104</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         antecedents  \\\n",
       "0                           (REGENCY TEA PLATE PINK)   \n",
       "1  (POPPY'S PLAYHOUSE LIVINGROOM, POPPY'S PLAYHOU...   \n",
       "2  (PINK REGENCY TEACUP AND SAUCER, ROSES REGENCY...   \n",
       "\n",
       "                         consequents  antecedent support  consequent support  \\\n",
       "0          (REGENCY TEA PLATE GREEN)            0.012092            0.014576   \n",
       "1        (POPPY'S PLAYHOUSE KITCHEN)            0.011067            0.018678   \n",
       "2  (GREEN REGENCY TEACUP AND SAUCER)            0.014306            0.037303   \n",
       "\n",
       "    support  confidence       lift  leverage  conviction  \n",
       "0  0.010905    0.901786  61.869180  0.010729   10.033411  \n",
       "1  0.010041    0.907317  48.575553  0.009834   10.587943  \n",
       "2  0.012902    0.901887  24.177353  0.012369    9.812104  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rule_conf_p9 = arule(frequent_itemsets, metric=\"confidence\", min_threshold=0.9)\n",
    "rule_conf_p9"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "47fca001-2e05-453a-970a-0155a1c83fbd",
   "metadata": {},
   "source": [
    "**Your answer:**\n",
    "\n",
    "The number of rules decreases when we make our criterion(confidence) stricter(from 0.6 to 0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1920ddaf-f961-4c50-b34b-d5d338223675",
   "metadata": {},
   "source": [
    "#### **b(v)** \n",
    "From the association rules that satisfy the confidence threshold 0.6, select and show the two rules with the highest lift. What do you notice if you compare the two rules with each other?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4110f530-8c31-4be4-96f2-aaa5d4595130",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>antecedents</th>\n",
       "      <th>consequents</th>\n",
       "      <th>antecedent support</th>\n",
       "      <th>consequent support</th>\n",
       "      <th>support</th>\n",
       "      <th>confidence</th>\n",
       "      <th>lift</th>\n",
       "      <th>leverage</th>\n",
       "      <th>conviction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>(REGENCY TEA PLATE PINK)</td>\n",
       "      <td>(REGENCY TEA PLATE GREEN)</td>\n",
       "      <td>0.012092</td>\n",
       "      <td>0.014576</td>\n",
       "      <td>0.010905</td>\n",
       "      <td>0.901786</td>\n",
       "      <td>61.86918</td>\n",
       "      <td>0.010729</td>\n",
       "      <td>10.033411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>(REGENCY TEA PLATE GREEN)</td>\n",
       "      <td>(REGENCY TEA PLATE PINK)</td>\n",
       "      <td>0.014576</td>\n",
       "      <td>0.012092</td>\n",
       "      <td>0.010905</td>\n",
       "      <td>0.748148</td>\n",
       "      <td>61.86918</td>\n",
       "      <td>0.010729</td>\n",
       "      <td>3.922574</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  antecedents                consequents  antecedent support  \\\n",
       "53   (REGENCY TEA PLATE PINK)  (REGENCY TEA PLATE GREEN)            0.012092   \n",
       "54  (REGENCY TEA PLATE GREEN)   (REGENCY TEA PLATE PINK)            0.014576   \n",
       "\n",
       "    consequent support   support  confidence      lift  leverage  conviction  \n",
       "53            0.014576  0.010905    0.901786  61.86918  0.010729   10.033411  \n",
       "54            0.012092  0.010905    0.748148  61.86918  0.010729    3.922574  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# your code\n",
    "\n",
    "rules_conf_p6_desc = rule_conf_p6.sort_values(by='lift', ascending=False)\n",
    "\n",
    "# Select the top 3 rows\n",
    "top_2 = rules_conf_p6_desc.head(2)\n",
    "top_2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "743f10f9-2067-4d68-9510-33a5699e4ae8",
   "metadata": {},
   "source": [
    "**Your answer:**\n",
    "\n",
    "These two rules are the exact complements of each other. Between the two items Regency Tea Plates Pink/Green, buying one item automatically supports buying the other. However the confidence and conviction is higher in the first rule."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcc2b990-3a2d-409c-907a-de7a812aa687",
   "metadata": {},
   "source": [
    "#### **b(vi)** \n",
    "\n",
    "In the analysis tasks in **b)**, an itemset consisted of items that had the same invoice number (same transaction items). Thus, if an itemset was frequent, it meant that the items in it were frequently purchased together.\n",
    "An association rule $A \\Rightarrow B$ meant that if items in $A$ are purchased, then the items in $B$ are also purchased in that same transaction.\n",
    "\n",
    "Suppose that we would repeat the analysis, but this time, the itemsets would consist of items having the same \"CustomerID\" (bought from the same customer). Interpret the meaning of the frequent itemsets and association rules for this kind of itemsets."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c7269908-782f-4900-a306-7507b7afd115",
   "metadata": {},
   "source": [
    "**Your answer:**\n",
    "\n",
    "- Right now we are looking at items bought at the same time - having the same Invoice ID.\n",
    "- If we consider Customer ID, then we would be looking at items bought by the same customer, regardless of time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a438848-5297-4ca8-9e09-cf414903197b",
   "metadata": {},
   "source": [
    "### c) Sequence Mining (3 pts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71b2c05a-2fa5-4150-b5ce-895d9f1078cc",
   "metadata": {},
   "source": [
    "For this task, the dataset used is `retail_sequences.csv`. Run the cell below to save the dataset under the dataframe `retail_sequences`. Each row in the dataframe corresponds to a unique customer (from the retail dataset). The \"Customer\" column contains the customer ID, whereas the \"Sequence\" column contains the sequence of itemsets  purchased by that customer.  Each value of \"Sequence\" is a sequence (list) of itemsets $<I_1, I_2, ..., I_n>$. The items within the same itemset (list without duplicates) $I_i$ were purchased together (they had the same invoice number). The itemsets are ordered by the timestamp of the transaction (value of InvoiceDate)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f5764854-1c5e-44a8-b83a-c1bed469e9fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "retail_sequences = pd.read_csv('retail_sequences.csv', converters={'Sequence': pd.eval})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6769f289-6619-41f4-855d-d5a690009e7b",
   "metadata": {
    "tags": []
   },
   "source": [
    "Given is the sequence *s= <{'lunch bag cars blue'}, {'herb marker rosemary','herb marker thyme'}, {'wooden star christmas scandinavian'}>*. Compute the support count of that sequence, that is, compute the number of customers whose corresponding itemset sequence contains it. Display its support count and the IDs of those customers.\n",
    "\n",
    "*Hint: In the dataset provided, all product names are unified. They are all lowercase and have no trailing spaces.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2ff9fcd7-4dc8-48a9-8ae0-5c8f0e0d0f81",
   "metadata": {},
   "outputs": [],
   "source": [
    "s = [['lunch bag cars blue'], ['herb marker rosemary', 'herb marker thyme'] ,['wooden star christmas scandinavian']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "006414b7-f6eb-4ea6-8d76-9ac57408501b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [[white hanging heart t-light holder, white me...\n",
       "1    [[assorted colour bird ornament, poppy's playh...\n",
       "2    [[alarm clock bakelike pink, alarm clock bakel...\n",
       "3    [[paper chain kit 50's christmas], [biscuit ti...\n",
       "4    [[victorian sewing box large], [victorian sewi...\n",
       "Name: Sequence, dtype: object"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# your code\n",
    "\n",
    "retail_sequences.Sequence.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bb34ba2-6c45-4dc7-97f1-0194de53b3f5",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Question 4: Text Mining (12 pts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "abb4c2cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "from nltk.lm.preprocessing import padded_everygram_pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85c87d2b-a192-496d-ae0e-52865d498d72",
   "metadata": {},
   "source": [
    "### F.R.I.E.N.D.S."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "725586d9-d73b-40bd-938d-e62dcae05fee",
   "metadata": {},
   "source": [
    "In this task we will use the script from the well-known series \"F.R.I.E.N.D.S.\". We will apply feature extraction methods to map the line of each main character onto a vector of a vector space. Then we will train a classifier whose aim will be to predict the name of the character given a particular line from the script.\n",
    "In the end, we will train language models using N-grams and produce fake sentences for each of the main characters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "945cdb03-6f30-4b39-93b5-11ae64b60c4e",
   "metadata": {},
   "source": [
    "### a) Data Loading and Preprocessing (4 pts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "172ce8b2-8381-4447-8c70-8a1f66fa4b17",
   "metadata": {},
   "source": [
    "#### **a(i)** \n",
    "Import the file `FRIENDS.csv` and save it into a dataframe named `friends_df`. Note that the dataframe must contain two columns: one indicating the character's name and one containing a line from the script. Display the first few lines from the dataframe.\n",
    "\n",
    "<i>FYI: The script has been filtered so that it only contains lines from the main characters. The order of the lines in the data is the same as the order of the lines in the original script. Metadata and scene descriptions have been removed. Your corpus consists of all the lines contained in the data. Each row's \"line\" value is a single document. </i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d249dbeb-b7c9-48f3-8fb8-546d052daac1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>character</th>\n",
       "      <th>line</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Monica</td>\n",
       "      <td>There's nothing to tell! He's just some guy I...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Joey</td>\n",
       "      <td>C'mon  you're going out with the guy! There's...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Chandler</td>\n",
       "      <td>All right Joey  be nice.  So does he have a h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Phoebe</td>\n",
       "      <td>Wait  does he eat chalk?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Phoebe</td>\n",
       "      <td>Just  'cause  I don't want her to go through ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  character                                               line\n",
       "0    Monica   There's nothing to tell! He's just some guy I...\n",
       "1      Joey   C'mon  you're going out with the guy! There's...\n",
       "2  Chandler   All right Joey  be nice.  So does he have a h...\n",
       "3    Phoebe                           Wait  does he eat chalk?\n",
       "4    Phoebe   Just  'cause  I don't want her to go through ..."
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# your code\n",
    "import pandas as pd\n",
    "\n",
    "friends_df = pd.read_csv('FRIENDS.csv')\n",
    "\n",
    "friends_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf8ecf98-6140-4c73-9350-08060d26deee",
   "metadata": {},
   "source": [
    "#### **a(ii)**  \n",
    "Plot the line count distribution among the six main characters (the six possible values of the column \"character\"). For example, show a plot containing one bar for each character whose height reflects the number of lines in `friends_df`. Briefly comment on the plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "534f1199-6f25-4e00-83c7-8aefbbe75250",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>character</th>\n",
       "      <th>number of lines</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Chandler</td>\n",
       "      <td>7687</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Joey</td>\n",
       "      <td>7573</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Monica</td>\n",
       "      <td>7651</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Phoebe</td>\n",
       "      <td>6833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Rachel</td>\n",
       "      <td>8506</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Ross</td>\n",
       "      <td>8264</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  character  number of lines\n",
       "0  Chandler             7687\n",
       "1      Joey             7573\n",
       "2    Monica             7651\n",
       "3    Phoebe             6833\n",
       "4    Rachel             8506\n",
       "5      Ross             8264"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# your code\n",
    "\n",
    "character_counts = friends_df.groupby('character').size().reset_index(name='number of lines')\n",
    "\n",
    "character_counts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d81b6fe9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\DELL\\anaconda3\\envs\\env-IDS2022-23\\lib\\site-packages\\seaborn\\rcmod.py:82: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "  if LooseVersion(mpl.__version__) >= \"3.0\":\n",
      "c:\\Users\\DELL\\anaconda3\\envs\\env-IDS2022-23\\lib\\site-packages\\setuptools\\_distutils\\version.py:351: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "  other = LooseVersion(other)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<AxesSubplot: xlabel='character', ylabel='number of lines'>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "sns.barplot(x='character', y='number of lines', data=character_counts)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "fe800c4f-7a8b-4c45-9484-ffeedb161a2e",
   "metadata": {},
   "source": [
    "**Your answer:**\n",
    "Rachel and Ross have spoken the most number of lines, Pheoebe has the least, while the rest of the three are more or less equal."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56c20766-da32-4e64-99af-1f8eb1a466e1",
   "metadata": {},
   "source": [
    "#### **a(iii)**  \n",
    "Create a corpus named `corpus` such that each document in the corpus corresponds to exactly one row's \"line\" in `friends_df`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a677e05f-51c6-431d-933f-961089327685",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\" There's nothing to tell! He's just some guy I work with!\",\n",
       " \" C'mon  you're going out with the guy! There's gotta be something wrong with him!\",\n",
       " ' All right Joey  be nice.  So does he have a hump? A hump and a hairpiece?',\n",
       " ' Wait  does he eat chalk?',\n",
       " \" Just  'cause  I don't want her to go through what I went through with Carl- oh!\"]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# your code\n",
    "corpus = friends_df['line'].tolist()\n",
    "# corpus = friends_df[['character', 'line']].values.tolist()\n",
    "\n",
    "corpus[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6a5e192-46e1-4c64-92d2-387c83e8d4c2",
   "metadata": {},
   "source": [
    "#### **a(iv)**  \n",
    "Write a function called `my_preprocessor` which, given a string, returns another string after tokenization, stopword removal and lemmatization have been applied. The remaining terms (tokens after stopword removal and lemmatization has been applied) should be joined in the same string using space ' '."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5365fc69-a32d-4b8a-a142-0ec4ae8aa988",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\DELL\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# your code\n",
    "\n",
    "import gensim\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
    "from nltk.stem.porter import *\n",
    "nltk.download('omw-1.4')\n",
    "\n",
    "stopwords = set(STOPWORDS)\n",
    "# print(stopwords)\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c3f753aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_preprocessor(line):\n",
    "    tokens = simple_preprocess(line)\n",
    "\n",
    "    tokens = [token for token in tokens if token not in stopwords]\n",
    "    # print(tokens)\n",
    "\n",
    "    lemmatized_tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "    return ' '.join(lemmatized_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "905725ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " There's nothing to tell! He's just some guy I work with!\n",
      "tell guy work\n"
     ]
    }
   ],
   "source": [
    "print(corpus[0])\n",
    "print(my_preprocessor(corpus[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b173fb1e-44ef-4e72-a087-2cc67d5464b1",
   "metadata": {},
   "source": [
    "#### **a(v)**   \n",
    "You must apply your preprocessor `my_preprocessor` on each line contained in the `corpus`. Create a preprocessed corpus named `corpus_p` which contains the same lines as `corpus` after the preprocessor `my_preprocessor` has been applied."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d46ad877-6f7d-45af-902a-313efaf6e9b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\" There's nothing to tell! He's just some guy I work with!\", \" C'mon  you're going out with the guy! There's gotta be something wrong with him!\", ' All right Joey  be nice.  So does he have a hump? A hump and a hairpiece?', ' Wait  does he eat chalk?', \" Just  'cause  I don't want her to go through what I went through with Carl- oh!\"]\n",
      "['tell guy work', 'mon going guy gotta wrong', 'right joey nice hump hump hairpiece', 'wait eat chalk', 'cause want went carl oh']\n"
     ]
    }
   ],
   "source": [
    "# your code\n",
    "print(corpus[:5])\n",
    "corpus_p = [my_preprocessor(line) for line in corpus]\n",
    "print(corpus_p[:5])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b89f6a33-d791-493d-b9a3-809cfad2d694",
   "metadata": {},
   "source": [
    "#### **a(vi)**   \n",
    "Split the `friends_df` dataset from the previous task into training (80%) and test (20%) data preserving the distribution based on the \"character\" value. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ffae634d-d8d3-4057-84a8-aa4117cf73dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>character</th>\n",
       "      <th>line</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Monica</td>\n",
       "      <td>There's nothing to tell! He's just some guy I...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Joey</td>\n",
       "      <td>C'mon  you're going out with the guy! There's...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Chandler</td>\n",
       "      <td>All right Joey  be nice.  So does he have a h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Phoebe</td>\n",
       "      <td>Wait  does he eat chalk?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Phoebe</td>\n",
       "      <td>Just  'cause  I don't want her to go through ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  character                                               line\n",
       "0    Monica   There's nothing to tell! He's just some guy I...\n",
       "1      Joey   C'mon  you're going out with the guy! There's...\n",
       "2  Chandler   All right Joey  be nice.  So does he have a h...\n",
       "3    Phoebe                           Wait  does he eat chalk?\n",
       "4    Phoebe   Just  'cause  I don't want her to go through ..."
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# your code\n",
    "friends_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "16cc3d73",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = friends_df.drop('character', axis=1)\n",
    "y = friends_df['character']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2fa33bfb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>line</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>27569</th>\n",
       "      <td>Okay. Okay fine  y’know what? We will let Ros...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14664</th>\n",
       "      <td>Ewwwww!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22154</th>\n",
       "      <td>Phoebe’s…</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7802</th>\n",
       "      <td>Umm  you’ve got some on your pants.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11255</th>\n",
       "      <td>It's okay. How's the soup?</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    line\n",
       "27569   Okay. Okay fine  y’know what? We will let Ros...\n",
       "14664                                            Ewwwww!\n",
       "22154                                          Phoebe’s…\n",
       "7802                 Umm  you’ve got some on your pants.\n",
       "11255                         It's okay. How's the soup?"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.head()\n",
    "# y_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3d998c5-75fc-4673-bb69-2a4a644b8681",
   "metadata": {},
   "source": [
    "#### **a(vii)**   \n",
    "Similar to **a(iii)**, for the training data and the test data, create two corpora named `corpus_train` and `corpus_test` respectively. Each document in the training (test) corpus must correspond to exactly one row's \"line\" value in the corresponding training (test) dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ab8fa13d-599d-4c60-b747-bde77c9e99d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\" He's probably in his room with his current girlfriend Charlie. That's the situation as we know it... \",\n",
       " ' Ohhhh ',\n",
       " ' We live together. You’re having our baby. I’m not gonna see anybody else. Are you-are you sure you don’t want something more?',\n",
       " \"  O-kay... I mean  don't I deserve anything? I mean  a few tears  a cursory hug?  NOT FROM YOU! \",\n",
       " \" Ooh  and it's so nice having this little sink here...\"]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# your code\n",
    "corpus_train = X_train['line'].tolist()\n",
    "corpus_test = X_test['line'].tolist()\n",
    "\n",
    "# corpus_train[:5]\n",
    "corpus_test[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b8945a0-a108-4380-b0fa-aec46768c284",
   "metadata": {},
   "source": [
    "### b) Set of Words (4.5 pts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a862af1a-83a6-46fd-ab0e-48a55d2667e9",
   "metadata": {},
   "source": [
    "#### **b(i)**  \n",
    "We want to encode our text in such a way that for each word in the vocabulary, we are only interested in whether the word appears or not in a given document. Create such a Set of Words encoding for the whole corpus `corpus`. Use the previously defined preprocessor `my_preprocessor` as preprocessor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "000b068a-d85d-405a-a6b2-b463d63afa70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code\n",
    "\n",
    "corpus_preprocessed = [my_preprocessor(line) for line in corpus]\n",
    "\n",
    "cv = CountVectorizer()\n",
    "\n",
    "cv_encoding = cv.fit_transform(corpus_preprocessed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "11a1088c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'scipy.sparse._csr.csr_matrix'>\n",
      "  (0, 9749)\t1\n",
      "  (0, 4211)\t1\n",
      "  (0, 10927)\t1\n",
      "  (1, 4211)\t1\n",
      "  (1, 6225)\t1\n",
      "  (1, 4027)\t1\n",
      "  (1, 4071)\t1\n",
      "  (1, 10972)\t1\n",
      "  (2, 8094)\t1\n",
      "  (2, 5082)\t1\n",
      "  (2, 6499)\t1\n",
      "  (2, 4652)\t2\n",
      "  (2, 4239)\t1\n",
      "  (3, 10590)\t1\n",
      "  (3, 2946)\t1\n",
      "  (3, 1551)\t1\n",
      "  (4, 1507)\t1\n",
      "  (4, 10626)\t1\n",
      "  (4, 10725)\t1\n",
      "  (4, 1449)\t1\n",
      "  (4, 6680)\t1\n"
     ]
    }
   ],
   "source": [
    "print(type(cv_encoding))\n",
    "print(cv_encoding[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e607c40e-1b8a-4bda-9dc2-0cc4f5872452",
   "metadata": {},
   "source": [
    "#### **b(ii)**  \n",
    "Pick one (any) of the lines of the \"line\" column in the `friends_df` dataset. Display the line in:\n",
    "    1) its original form, \n",
    "    2) its preprocessed version (the result contained in `corpus_p` after applying `my_preprocessor`), and \n",
    "    3) its encoding computed by the Set of Words method. This can be either an array (a vector) or a scipy matrix. \n",
    "Briefly comment on the Set of Words encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "152e71ca-9019-47b1-a827-b7227e85a20d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Original Line\n",
      "0     There's nothing to tell! He's just some guy I...\n",
      "Name: line, dtype: object\n",
      "\n",
      " Preprocessed\n",
      "tell guy work\n",
      "\n",
      "Encoded\n",
      "  (0, 9749)\t1\n",
      "  (0, 4211)\t1\n",
      "  (0, 10927)\t1\n"
     ]
    }
   ],
   "source": [
    "# your code\n",
    "print(\"\\n Original Line\")\n",
    "print(friends_df.head(1).line)\n",
    "\n",
    "print(\"\\n Preprocessed\")\n",
    "print(corpus_p[0])\n",
    "\n",
    "print(\"\\nEncoded\")\n",
    "print(cv_encoding[0])\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "16b431ce-c9ef-46fd-80f9-5c0ee685770b",
   "metadata": {},
   "source": [
    "**Your answer:**\n",
    "\n",
    "The set of words encoding leads to a sparse matrix because we have a huge vocab and sentences would usually contain very few words. Also, with stop word removal, lemmatization, this decreases futher. We are just stornig frequency of occurence of words which may not lead to a very good model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c704be66-78e8-40b5-9f88-3df1939000e6",
   "metadata": {},
   "source": [
    "#### **b(iii)**  \n",
    "Create a Set of Words encoding based only on the documents in `corpus_train`. Use the previously defined preprocessor `my_preprocessor` as a preprocessor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "421019ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' Okay. Okay fine  y’know what? We will let Ross and Joey decide.   Hiiiii  Ross! Sweetie.',\n",
       " ' Ewwwww!',\n",
       " ' Phoebe’s…',\n",
       " ' Umm  you’ve got some on your pants.',\n",
       " \" It's okay. How's the soup?\"]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus_train[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "bbedb7eb-192a-4f0d-b590-9cf3cf8a813d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code\n",
    "corpus_train_preprocessed = [my_preprocessor(line) for line in corpus_train]\n",
    "\n",
    "cv_train_encoded = cv.transform(corpus_train_preprocessed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d26cad99-701b-4e56-a020-5e2d1ef04a4d",
   "metadata": {},
   "source": [
    "#### **b(iv)**  \n",
    "In this task, we will use an SGD (Stochastic Gradient Descend) classifier to predict the character given a line from the corpus. Train the classifier on the Set of Words encoding of training corpus `corpus_train` using the character as the target feature and 'log_loss' as the loss function. Apply the classifier on the Set of Words encodings of both the training corpus and the test corpus `corpus_train`. Show its accuracy for both the training corpus and the test corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "5aeffa1d-bb0b-4112-846c-74cde3e217dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>SGDClassifier(loss=&#x27;log_loss&#x27;)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">SGDClassifier</label><div class=\"sk-toggleable__content\"><pre>SGDClassifier(loss=&#x27;log_loss&#x27;)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "SGDClassifier(loss='log_loss')"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# your code\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "clf = SGDClassifier(loss='log_loss')\n",
    "clf.fit(cv_train_encoded, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f401ba02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 0.41737658219343743\n"
     ]
    }
   ],
   "source": [
    "predicted_labels_train = clf.predict(cv_train_encoded)\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_train = accuracy_score(y_train, predicted_labels_train)\n",
    "print('Train Accuracy:', accuracy_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "fa54ea03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.2866817155756208\n"
     ]
    }
   ],
   "source": [
    "corpus_test_preprocessed = [my_preprocessor(line) for line in corpus_test]\n",
    "cv_test_encoded = cv.transform(corpus_test_preprocessed)\n",
    "\n",
    "predicted_labels_test = clf.predict(cv_test_encoded)\n",
    "\n",
    "accuracy_test = accuracy_score(y_test, predicted_labels_test)\n",
    "print('Test Accuracy:', accuracy_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c77974b1-4fb7-4fa9-9ef5-40764d4a0a28",
   "metadata": {},
   "source": [
    "#### **b(v)**  \n",
    "Briefly comment on the accuracy of the classifier compared to the expected accuracy of a random guesser (here: a model that simply guesses each character according to a distribution based on the line count). Use the line count distribution shown in **a(ii)** to reason about the approximate accuracy of the random guesser."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "86b7d8d1-3375-49b1-b69e-a4e2da8c184e",
   "metadata": {},
   "source": [
    "**Your answer:**\n",
    "\n",
    "We got an accuracy of 0.28, which is very low. \n",
    "I think if we had used a random guesser, it would've given us an accuracy of 1/6, which is around 0.16.\n",
    "Acutally, considering the line cound distribution from **a(ii)**, it will be even **less than 0.16** as it would predict Pheobe's lines more than it needs to, and Rachel/Ross's lines less than it needs to"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afe69290-07c2-4f7f-8693-d05884b98963",
   "metadata": {},
   "source": [
    "#### **b(vi)**  \n",
    "Pick two lines from the dataset `friends_df`. Predict their character by applying the SGD classifier from **b(iv)** to their Set of Words encodings. Show the original lines, their original characters and the predicted characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "3a7621f1-8fc9-4606-9c19-0fb007e13b7b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>character</th>\n",
       "      <th>line</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Monica</td>\n",
       "      <td>There's nothing to tell! He's just some guy I...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Joey</td>\n",
       "      <td>C'mon  you're going out with the guy! There's...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  character                                               line\n",
       "0    Monica   There's nothing to tell! He's just some guy I...\n",
       "1      Joey   C'mon  you're going out with the guy! There's..."
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# your code\n",
    "\n",
    "friends_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "5e373cdf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Monica', 'Rachel'], dtype='<U8')"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines = friends_df.head(2)['line'].to_list()\n",
    "lines_prep = [my_preprocessor(line) for line in lines]\n",
    "lines_enc = cv.transform(lines_prep)\n",
    "predictions = clf.predict(lines_enc)\n",
    "\n",
    "predictions"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "55d3733f",
   "metadata": {},
   "source": [
    "| Line | Original | Predicted |\n",
    "|--------------|-----------|------------|\n",
    "1 | Monica | Monica |\n",
    "2 | Joey | Rachel |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eccc83fe-08a7-4109-a68d-765799874a83",
   "metadata": {},
   "source": [
    "### c) Doc2Vec (1.5 pts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cca937f-cc73-496b-90df-3d571b4ebd3b",
   "metadata": {},
   "source": [
    "#### **c(i)**  \n",
    "In this part, we want to encode the lines using Doc2Vec. Create a Doc2Vec embedding based on the documents in the preprocessed corpus `corpus_p`. Set the vector dimension to 10 and min_count to 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "9bcd41ee-d39a-43a3-a7c0-eb2e6f1c9a29",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['tell guy work',\n",
       " 'mon going guy gotta wrong',\n",
       " 'right joey nice hump hump hairpiece',\n",
       " 'wait eat chalk',\n",
       " 'cause want went carl oh']"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# your code\n",
    "corpus_p[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "b2a47dd9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['mon', 'going', 'guy', 'gotta', 'wrong']"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus_p[1].split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "b2cacc02",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[TaggedDocument(words=['tell', 'guy', 'work'], tags=[0]),\n",
       " TaggedDocument(words=['mon', 'going', 'guy', 'gotta', 'wrong'], tags=[1]),\n",
       " TaggedDocument(words=['right', 'joey', 'nice', 'hump', 'hump', 'hairpiece'], tags=[2]),\n",
       " TaggedDocument(words=['wait', 'eat', 'chalk'], tags=[3]),\n",
       " TaggedDocument(words=['cause', 'want', 'went', 'carl', 'oh'], tags=[4])]"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tagged_data = [TaggedDocument(words=sentence.split(), tags=[i]) for i, sentence in enumerate(corpus_p)]\n",
    "\n",
    "tagged_data[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "fb0b975c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_d2v = Doc2Vec(vector_size=10, min_count=3, epochs=10)\n",
    "model_d2v.build_vocab(tagged_data)\n",
    "model_d2v.train(tagged_data, total_examples=model_d2v.corpus_count, epochs=model_d2v.epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "fe70ff75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " That's right.\n",
      "right\n",
      "[ 0.04455074  0.01757851 -0.00498689 -0.01740584 -0.00626002 -0.02545759\n",
      "  0.04262686  0.0540191  -0.01110759  0.02866038]\n"
     ]
    }
   ],
   "source": [
    "print(corpus[10])\n",
    "print(corpus_p[10])\n",
    "vector = model_d2v.infer_vector(corpus_p[10].split())\n",
    "print(vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fee6650f-1ca5-47aa-adfd-56c33c6dc4fd",
   "metadata": {},
   "source": [
    "#### **c(ii)**  \n",
    "Pick one (any) line from the dataset `friends_df`. Display the line and the character saying it. Find its most similar line w.r.t. the Doc2Vec encoding and display the original line and its corresponding character. Do the lines belong to the same character?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "7f543244-67a5-4d00-aff1-9a5a05518ad8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" My brother's going through that right now  he's such a mess. How did you get through it?\""
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# your code\n",
    "r = 121\n",
    "random_row = list(friends_df.iloc[r, :])\n",
    "random_line = random_row[1]\n",
    "random_line\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "da4f25e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "brother going right mess\n",
      "[-0.06795917 -0.03485595 -0.02265333  0.00860751  0.0208063  -0.03854308\n",
      "  0.01320639  0.07622895 -0.02651561 -0.04892675]\n"
     ]
    }
   ],
   "source": [
    "line_p = my_preprocessor(random_line)\n",
    "print(line_p)\n",
    "input_vector = model_d2v.infer_vector(line_p.split())\n",
    "print(input_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "60d431ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mon going guy gotta wrong\n",
      "[ 0.03796047 -0.06983138  0.03551874  0.03654374  0.00079782 -0.00086519\n",
      "  0.15526578 -0.01220052 -0.01970057 -0.02679877]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "cannot convert dictionary update sequence element #0 to a sequence",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\DELL\\Documents\\Media Informatics\\Courses\\Sem 5\\Intro to DS\\Assignment 2\\assignment2.ipynb Cell 148\u001b[0m in \u001b[0;36m<cell line: 7>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/DELL/Documents/Media%20Informatics/Courses/Sem%205/Intro%20to%20DS/Assignment%202/assignment2.ipynb#Y464sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m this_vector \u001b[39m=\u001b[39m model_d2v\u001b[39m.\u001b[39minfer_vector(sentence_split)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/DELL/Documents/Media%20Informatics/Courses/Sem%205/Intro%20to%20DS/Assignment%202/assignment2.ipynb#Y464sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m \u001b[39mprint\u001b[39m(vector)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/DELL/Documents/Media%20Informatics/Courses/Sem%205/Intro%20to%20DS/Assignment%202/assignment2.ipynb#Y464sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m similarity \u001b[39m=\u001b[39m cossim(input_vector, this_vector)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/DELL/Documents/Media%20Informatics/Courses/Sem%205/Intro%20to%20DS/Assignment%202/assignment2.ipynb#Y464sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m \u001b[39mif\u001b[39;00m similarity \u001b[39m>\u001b[39m max_similarity:\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/DELL/Documents/Media%20Informatics/Courses/Sem%205/Intro%20to%20DS/Assignment%202/assignment2.ipynb#Y464sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m     max_similarity \u001b[39m=\u001b[39m similarity\n",
      "File \u001b[1;32mc:\\Users\\DELL\\anaconda3\\envs\\env-IDS2022-23\\lib\\site-packages\\gensim\\matutils.py:810\u001b[0m, in \u001b[0;36mcossim\u001b[1;34m(vec1, vec2)\u001b[0m\n\u001b[0;32m    792\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcossim\u001b[39m(vec1, vec2):\n\u001b[0;32m    793\u001b[0m     \u001b[39m\"\"\"Get cosine similarity between two sparse vectors.\u001b[39;00m\n\u001b[0;32m    794\u001b[0m \n\u001b[0;32m    795\u001b[0m \u001b[39m    Cosine similarity is a number between `<-1.0, 1.0>`, higher means more similar.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    808\u001b[0m \n\u001b[0;32m    809\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 810\u001b[0m     vec1, vec2 \u001b[39m=\u001b[39m \u001b[39mdict\u001b[39;49m(vec1), \u001b[39mdict\u001b[39m(vec2)\n\u001b[0;32m    811\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m vec1 \u001b[39mor\u001b[39;00m \u001b[39mnot\u001b[39;00m vec2:\n\u001b[0;32m    812\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39m0.0\u001b[39m\n",
      "\u001b[1;31mTypeError\u001b[0m: cannot convert dictionary update sequence element #0 to a sequence"
     ]
    }
   ],
   "source": [
    "from gensim.matutils import cossim\n",
    "\n",
    "max_similarity = -1\n",
    "closest_sentence = \"\"\n",
    "closest_sentence_index = 0\n",
    "\n",
    "for i in range(len(corpus_p)):\n",
    "    sentence = corpus_p[i]\n",
    "    sentence_split = sentence.split()\n",
    "    if (len(sentence_split)>3):  #min_count = 3\n",
    "        print(sentence)\n",
    "        this_vector = model_d2v.infer_vector(sentence_split)\n",
    "        print(vector)\n",
    "        similarity = cossim(input_vector, this_vector)\n",
    "        if similarity > max_similarity:\n",
    "            max_similarity = similarity\n",
    "            closest_sentence = sentence\n",
    "            closest_sentence_index = i\n",
    "\n",
    "# print(\"Closest sentence preprocessed:\", closest_sentence)\n",
    "print(\"Closest sentence:\", corpus[closest_sentence_index])\n",
    "print(\"Character :\", list(friends_df.iloc[closest_sentence_index, :])[0])\n",
    "\n",
    "\n",
    "# Something wrong with gensim cosine similarity. I can't figure it out. Therefore used sklearn in the next cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "fab45b76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Closest sentence preprocessed: hey listen let ask believe soul mate\n",
      "Closest sentence:  Hey listen let me ask you  do you believe in soul mates?\n",
      "Character : Phoebe\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "max_similarity = -1\n",
    "closest_sentence = \"\"\n",
    "closest_sentence_index = 0\n",
    "\n",
    "for i in range(len(corpus_p)):\n",
    "    sentence = corpus_p[i]\n",
    "    sentence_split = sentence.split()\n",
    "    if (sentence!=line_p and len(sentence_split)>3):  # not the same sentence as input, min_count = 3\n",
    "        # print(sentence)\n",
    "        this_vector = model_d2v.infer_vector(sentence_split)\n",
    "        # print(this_vector)\n",
    "        similarity = cosine_similarity(input_vector.reshape(1, -1), this_vector.reshape(1, -1))\n",
    "        if similarity > max_similarity:\n",
    "            max_similarity = similarity\n",
    "            closest_sentence = sentence\n",
    "            closest_sentence_index = i\n",
    "\n",
    "print(\"Closest sentence preprocessed:\", closest_sentence)\n",
    "print(\"Closest sentence:\", corpus[closest_sentence_index])\n",
    "print(\"Character :\", list(friends_df.iloc[closest_sentence_index, :])[0])\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "437b009f-0d60-4f3c-857a-0ff2c62763af",
   "metadata": {},
   "source": [
    "**Your answer:**\n",
    "\n",
    "At first it returned the same sentence that we had passed as input, as it has the highest similarity.\n",
    "Then, after adding the condition to not let it happen, it worked.\n",
    "\n",
    "| Sentence | Character | Line |\n",
    "|--------------|-----------|------------|\n",
    "| Original | Monica | My brother's going through that right now  he's such a mess. How did you get through it? |\n",
    "|Closest |Pheobe|Hey listen let me ask you  do you believe in soul mates?|"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aae3f695-65a1-4477-8909-517ec5133e5c",
   "metadata": {},
   "source": [
    "### d) Language model using N-grams (2 pts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba2d9dad-ffc3-4ae0-8c31-7ac550656871",
   "metadata": {},
   "source": [
    "#### For the following tasks, use the `friends_df` data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5da82a84-2f4b-44e6-9138-deab47dbeb92",
   "metadata": {},
   "source": [
    "#### **d(i)**  \n",
    "For each character, create a corresponding corpus. Each corpus must be a list of documents. Each document corresponds to one \"line\" value of that character and it should be a list of terms. You must obtain this list of terms after applying preprocessing steps such as to lowercase, no punctuation, and tokenization to the original line. Do not perform stemming/lemmatization and/or stopword removal for this task.\n",
    "Display the corpus of one of the characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "de295c77-0ef1-423e-ac2d-2dd4241d22c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>character</th>\n",
       "      <th>line</th>\n",
       "      <th>preprocessed_line</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Monica</td>\n",
       "      <td>There's nothing to tell! He's just some guy I...</td>\n",
       "      <td>[there, nothing, to, tell, he, just, some, guy...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Joey</td>\n",
       "      <td>C'mon  you're going out with the guy! There's...</td>\n",
       "      <td>[mon, you, re, going, out, with, the, guy, the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Chandler</td>\n",
       "      <td>All right Joey  be nice.  So does he have a h...</td>\n",
       "      <td>[all, right, joey, be, nice, so, does, he, hav...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Phoebe</td>\n",
       "      <td>Wait  does he eat chalk?</td>\n",
       "      <td>[wait, does, he, eat, chalk]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Phoebe</td>\n",
       "      <td>Just  'cause  I don't want her to go through ...</td>\n",
       "      <td>[just, cause, don, want, her, to, go, through,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  character                                               line  \\\n",
       "0    Monica   There's nothing to tell! He's just some guy I...   \n",
       "1      Joey   C'mon  you're going out with the guy! There's...   \n",
       "2  Chandler   All right Joey  be nice.  So does he have a h...   \n",
       "3    Phoebe                           Wait  does he eat chalk?   \n",
       "4    Phoebe   Just  'cause  I don't want her to go through ...   \n",
       "\n",
       "                                   preprocessed_line  \n",
       "0  [there, nothing, to, tell, he, just, some, guy...  \n",
       "1  [mon, you, re, going, out, with, the, guy, the...  \n",
       "2  [all, right, joey, be, nice, so, does, he, hav...  \n",
       "3                       [wait, does, he, eat, chalk]  \n",
       "4  [just, cause, don, want, her, to, go, through,...  "
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# your code\n",
    "\n",
    "import pandas as pd\n",
    "from gensim.utils import simple_preprocess\n",
    "\n",
    "\n",
    "def preprocess(line):\n",
    "    line_p = simple_preprocess(line)\n",
    "    return line_p\n",
    "    # return ' '.join(word for word in line_p)\n",
    "\n",
    "\n",
    "friends_df = pd.read_csv('FRIENDS.csv')\n",
    "friends_df['preprocessed_line'] = friends_df['line'].apply(preprocess)\n",
    "\n",
    "friends_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "9ddcab55",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['there',\n",
       "  'nothing',\n",
       "  'to',\n",
       "  'tell',\n",
       "  'he',\n",
       "  'just',\n",
       "  'some',\n",
       "  'guy',\n",
       "  'work',\n",
       "  'with']]"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "monica_lines = friends_df.loc[friends_df['character'] == 'Monica']\n",
    "monica_lines_corpus = monica_lines['preprocessed_line'].tolist()\n",
    "monica_lines_corpus[:1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "ec1abd90",
   "metadata": {},
   "outputs": [],
   "source": [
    "joey_lines = friends_df.loc[friends_df['character'] == 'Joey']\n",
    "joey_lines_corpus = joey_lines['preprocessed_line'].tolist()\n",
    "\n",
    "chandler_lines = friends_df.loc[friends_df['character'] == 'Chandler']\n",
    "chandler_lines_corpus = chandler_lines['preprocessed_line'].tolist()\n",
    "\n",
    "phoebe_lines = friends_df.loc[friends_df['character'] == 'Phoebe']\n",
    "phoebe_lines_corpus = phoebe_lines['preprocessed_line'].tolist()\n",
    "\n",
    "rachel_lines = friends_df.loc[friends_df['character'] == 'Rachel']\n",
    "rachel_lines_corpus = rachel_lines['preprocessed_line'].tolist()\n",
    "\n",
    "ross_lines = friends_df.loc[friends_df['character'] == 'Ross']\n",
    "ross_lines_corpus = ross_lines['preprocessed_line'].tolist()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e162de2-271d-4031-858b-40f5dcbc5506",
   "metadata": {},
   "source": [
    "#### **d(ii)**  \n",
    "For each character separately, build a trigram language model using MLE. Use both right and left padding and learn each language model using the character's corpus from **d(ii)**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "838f330b-20b3-4c1a-af32-6a84d45330fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code\n",
    "monica_train_data, monica_padded_sents = padded_everygram_pipeline(3, monica_lines_corpus)\n",
    "\n",
    "from nltk.lm import MLE\n",
    "\n",
    "model_monica = MLE(3)\n",
    "model_monica.fit(monica_train_data, monica_padded_sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "3c2babf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "joey_train_data, joey_padded_sents = padded_everygram_pipeline(3, joey_lines_corpus)\n",
    "model_joey = MLE(3)\n",
    "model_joey.fit(joey_train_data, joey_padded_sents)\n",
    "\n",
    "chandler_train_data, chandler_padded_sents = padded_everygram_pipeline(3, chandler_lines_corpus)\n",
    "model_chandler = MLE(3)\n",
    "model_chandler.fit(chandler_train_data, chandler_padded_sents)\n",
    "\n",
    "phoebe_train_data, pheobe_padded_sents = padded_everygram_pipeline(3, phoebe_lines_corpus)\n",
    "model_phoebe = MLE(3)\n",
    "model_phoebe.fit(phoebe_train_data, pheobe_padded_sents)\n",
    "\n",
    "rachel_train_data, rachel_padded_sents = padded_everygram_pipeline(3, rachel_lines_corpus)\n",
    "model_rachel = MLE(3)\n",
    "model_rachel.fit(rachel_train_data, rachel_padded_sents)\n",
    "\n",
    "ross_train_data, ross_padded_sents = padded_everygram_pipeline(3, ross_lines_corpus)\n",
    "model_ross = MLE(3)\n",
    "model_ross.fit(ross_train_data, ross_padded_sents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e734c4b-2c31-4f19-9f43-4cf2983d8076",
   "metadata": {},
   "source": [
    "#### **d(iv)**  \n",
    "For each character, use the created trigram language model to generate a sentence of ten words. Display the sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "418add81-ad45-4712-905d-4ae71010f251",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Monica : ['know', 'he', 'too', 'charming', 'but', 'if', 'it', 'gets', 'little', 'extra']\n",
      "Joey : ['let', 'go', 'watch', 'it', 'at', 'work', 'got', 'me', 'for', 'sorry']\n",
      "Chandler : ['get', 'past', 'this', 'because', 'of', 'the', 'champagne', 'that', 'she', 'was']\n",
      "Pheobe : ['gonna', 'have', 'to', 'go', 'out', 'with', 'girl', '</s>', '</s>', '</s>']\n",
      "Rachel : ['like', 'going', 'on', 'with', 'you', 'guys', 'been', 'listening', 'this', 'entire']\n",
      "Ross : ['which', 'is', 'what', 'happens', 'when', 'people', 'live', 'on', 'the', 'figure']\n"
     ]
    }
   ],
   "source": [
    "# your code\n",
    "print(\"Monica :\", model_monica.generate(random_seed=101, num_words=10))\n",
    "print(\"Joey :\", model_joey.generate(random_seed=101, num_words=10))\n",
    "print(\"Chandler :\", model_chandler.generate(random_seed=11, num_words=10))\n",
    "print(\"Pheobe :\", model_phoebe.generate(random_seed=11, num_words=10))\n",
    "print(\"Rachel :\", model_rachel.generate(random_seed=101, num_words=10))\n",
    "print(\"Ross :\", model_ross.generate(random_seed=110, num_words=10))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "788da653-9059-4045-9341-da119995bc4c",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Question 5: Process Mining (22pts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75571aa7-6567-44de-86e3-27aba8d5d03f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pm4py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "470d33e5-5fea-4f90-baba-dfc4c7fc72bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pm4py.statistics.traces.generic.log import case_statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4472e969-94fb-47e1-bb32-6dcfd7f154b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pm4py.algo.conformance.tokenreplay import algorithm as token_based_replay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8da87cd3-a2f0-46ee-97e2-c3c3c9905140",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1fd03f7-1c78-45be-9fb1-018779e47fd5",
   "metadata": {
    "tags": []
   },
   "source": [
    "### a) Loading the Data and Basic Statistics (9pts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e696437-3c4a-4e5f-9725-b12d153d1688",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### **a(i)** \n",
    "Load the data **reimburse.csv** and create a PM4Py event log. In doing so, use the following column mapping:\n",
    " - *Activity* is the activity key\n",
    " - *Case* is the case ID\n",
    " - *Timestamp* is the timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d14d031-f77b-4b7c-807a-b352e2e4a7d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1cdcace-82db-451c-8d8a-2fe3d7033df2",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### **a(ii)** \n",
    "Compute and print the following basic information:\n",
    "- Number of events\n",
    "- Number of cases\n",
    "- Earliest timestamp\n",
    "- Latest timestamp\n",
    "- Number of trace variants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04d1b421-7308-4031-a5b2-bbf4cbfa18e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a45f421-ded8-4ebc-bfcd-bde5d3e3069b",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### **a(iii)** \n",
    "In process mining, multiple events that have the same timestamp can cause problems because the ordering of events (or even activities) becomes unclear. Moreover, they can indicate batching (i.e., one activity is executed for multiple cases simultaneously). Therefore, during your analysis, it is good to keep that in mind. To this end, compute the following statistics/answer the following questions:\n",
    "\n",
    "1. How many events occur almost at the same time (i.e., within less than 100ms as the preceding event. (Proceeding event in the *entire* event log)?\n",
    "2. Are there resources that complete two activities at the same time (within less than 100ms)?\n",
    "3. How many cases are there in which two activities are executed at the same time (i.e., two events that belong to the same case occur within less than 100ms)?\n",
    "\n",
    "*Hint: Depending on how you find the answers, be careful about event orderings.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70064816-159a-4092-92ec-d7d59e21c89f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c75d70c8-9b9a-4444-bd50-3e6d78c25bc6",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### **a(iv)** \n",
    "Provide a plot that shows the number of running cases (i.e., cases that have started but not yet finished) over time. You may assume that the log only contains complete traces. For each case that has started, its completion is the last observed event associated with that case. In case multiple cases start or end at the same time, you also generate multiple values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "522aaab1-e228-430a-bafd-3d1e82101aaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e9e3d73-265e-4161-8bfe-48a16643d42c",
   "metadata": {},
   "source": [
    "**Your answer**: *(Briefly describe the differences between the two models in about five sentences here.)*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e71e62b5-cc4e-4f3c-b836-08455477b93f",
   "metadata": {
    "tags": []
   },
   "source": [
    "### b) Discovery and Conformance Checking (9pts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fe3d450-49b1-479f-9e99-9760f8b9bd14",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### **b(i)** \n",
    "Mine a Petri net using Inductive Miner and display it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6417094f-6d92-4f1c-b63c-57588f16be5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d32b33dd-e8f5-46a7-be13-4821fbbf9e56",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### **b(ii)** \n",
    "Compute the fitness of the discovered Petri net using token-based replay.\n",
    "\n",
    "*Hint: PM4Py can directly (using the top-level API) compute the number of missing, remaining, consumed, and produced tokens. Based on these, you can, for example, compute the token-based replay fitness.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aa42fb9-c4f2-4bec-a89a-308bef0b026c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16ab43b6-d08b-4b09-8b96-54b911b9c34f",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### **b(iii)** \n",
    "Filter the log to contain only traces where *Register Low* occurs. How many traces does the resulting log `log_low` contain?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faba7573-fcc6-46a7-ae70-35111ba8e5ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "cc3ef414-44a5-4a73-906f-36f673cfb6d5",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### **b(iv)** \n",
    "Discover a Petri net for `log_low` and compute its fitness. How does this model differ from the model you discovered in *b(i)*? \n",
    "\n",
    "Suppose each of your produced process models is considered a 2-class classifier: provided a trace, it returns \"Yes\" if and only if the trace can be replayed by the model. Based on this perspective, how would the two process models compare in terms of precision?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cc94f06-ad65-408d-ad9e-e8b114838b50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74965d2a-c969-438d-b1f7-a8123142b789",
   "metadata": {},
   "source": [
    "**Your answer:** *(Briefly describe the difference between the two models here. About two sentences can be enough.*)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9833080-54f8-4e1b-baea-810962a98281",
   "metadata": {},
   "source": [
    "**Your answer**: *(Relate your observations to precision here.)*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1c73b63-a4a4-4438-a8e4-d2ebfe276051",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Conformance Diagnostics De-jure Model\n",
    "The process owner provides you a de-jure model (i.e., a model of the should-be process) and a slightly changed version of the so far considered event log. \n",
    "\n",
    "In this task, you will again apply conformance checking by means of token-based replay to provide diagnostics on deviations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "390eabfa-faab-4bea-88ae-7c39bcad1ba0",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### **b(v)** \n",
    "Load the Petri net *pn_conf.apnml*, the event log *log_conf.xes*, and provide the overall (i.e., model-based) token-based replay fitness score.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98673f6e-e70e-4bc5-969b-057e954c58e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4b101e2-9b42-481f-a5f2-3c066416a131",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### **b(vi)** \n",
    "To provide additional diagnostics on the deviations, compute the missing, consumed, produced and remaining number of tokens for **each place**. \n",
    "To do so, use the following *pm4py* code:\n",
    "    \n",
    "    from pm4py.algo.conformance.tokenreplay import algorithm as token_based_replay\n",
    "\ttbr_results, place_fitness, transition_fitness, notexisting_activities_in_model =\n",
    "    token_based_replay.apply(log_conf, net_conf, im_conf, fm_conf, parameters={\"enable_pltr_fitness\": True, \"disable_variants\": True})\n",
    "\n",
    "After running this line for log `log_conf`, Petri net `net_conf` with initial marking `im_conf` and final marking `fm_conf`, the variable `place_fitness` will contain the token counts for each place and trace. Therefore, you will only need aggregate over the traces.\n",
    "Print a table of the token counts per place. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9a01c3b-4be6-4ea3-a0c7-3572d5bd696f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e098ac37-4978-4520-9d39-51462a1f5038",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### **b(vii)** \n",
    "Consider the token counts per place and a few unfitting traces, which deviation(s) do you observe? Describe the deviation and briefly explain how it can be related to the token counts of the individual places. For example, activity *xy* is often missing resulting in a high number of missing tokens in place *p*.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02586824-a2d3-4d36-b181-6e70fc3e2b54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cec26df-0110-41a8-a427-eeeea611fd7c",
   "metadata": {},
   "source": [
    "**Your answer:** *(Describe the deviation(s). One sentence can already be enough.)*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49fdc361-bfa7-49d1-809f-4509b427d8da",
   "metadata": {},
   "source": [
    "**Your answer:** (*Relate the deviation(s) to the token counts of the individual places. Roughly five sentences can be enough for a precise description.*)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58dcdf9d-c227-4436-b9c3-d45c5b6f7de4",
   "metadata": {
    "tags": []
   },
   "source": [
    "### c) Analyzing Fraud (4pts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2048349d-ebf9-48cc-af0b-9da5a2828b23",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### **c(i)** \n",
    "Filter the event log so that it only contains traces where a fraud report is filled (occurrence of `Fill Fraud Report`). For theses traces, create a bar plot showing the number of products of a certain brand involved in the fraud. Describe the resulting plot.\n",
    "\n",
    "*Hint: Each case is associated with precisely one brand.*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b55a45be-a26c-4098-b23a-29b85102a715",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca63f184-9cb3-48a0-abd4-b4146f1ebcaa",
   "metadata": {},
   "source": [
    "**Your answer:** (*Describe the plot in two to three sentences.*)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c635a50b-a7d9-433c-96c8-aae526f9e459",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### **c(ii)**  \n",
    "The plot shows differences between brands. Discuss the result. Consider what you learned in Lecture 11 (association rules). Try to provide additional analysis results to underpin your discussion.\n",
    "\n",
    "*Hint: A very short additional analysis (i.e., a few lines of code) might already be sufficient.*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a32ba47d-c41f-418a-8b6b-b8176171b0a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code (for a short additional analysis)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81d49c56-6d9e-4b11-9fa4-d623f87a2f9a",
   "metadata": {},
   "source": [
    "**Your answer:** *(Relate your results to Lecture 11, approximately one short paragraph)*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63da1709-2452-4ac2-a162-ea5702d1694a",
   "metadata": {},
   "source": [
    "## Question 6 - Simpson's Paradox (3 pts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dd0ddaa-03b6-4195-923b-6ce0e73b02f0",
   "metadata": {},
   "source": [
    "### Sex Bias in Berkeley Graduate Admissions?\n",
    "\n",
    "In the Fall of 1973, the University of California at Berkeley released data about their graduate class. The data showed the major the applicant applied to, their self-reported gender (Male or Female), and whether or not they were accepted or rejected. The acceptance rates between men and women were different. This caused immediate concern in the public as people thought that Berkeley was biased against women."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04df6ab4-e21f-4265-8d40-077bff5d5baa",
   "metadata": {},
   "source": [
    "The \"Berkeley Dataset\" contains all 12,763 applicants to UC-Berkeley's graduate programs in Fall 1973. This dataset was published by UC-Berkeley researchers in an analysis to understand the possible gender bias in admissions.\n",
    "\n",
    "Dataset Variables:\n",
    "\n",
    "Year : number ➜ The application year (this value is always 1973)\n",
    "\n",
    "Major : string ➜: An anonymized major code (either A, B, C, D, E, F, or Other). The specific majors are unknown except that A-F are the six majors with the most applicants in Fall 1973\n",
    "\n",
    "Gender : string ➜ Applicant self-reported gender (either M or F)\n",
    "\n",
    "Admission: string ➜ Admission decision (either Rejected or Accepted)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "058897ab-2f92-4f71-b00c-ec3109d63687",
   "metadata": {},
   "source": [
    "**a)**\n",
    "Upload the data from the `berkeley.csv` file and load it into a dataframe named `data`. Display the first few lines from the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a34afacf-87c8-472b-86ae-728311145b18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b5bd1eb-0c70-4e11-b0b9-fa132dbebdd3",
   "metadata": {},
   "source": [
    "**b)** Remove the \"Year\" column as it does not contain any information in this dataset (all years are 1973.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db47b833-2b21-473e-a2ae-93edceb1478c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef777de3-cdcd-474d-ba69-d8c206bfd46e",
   "metadata": {},
   "source": [
    "**c)** For each of the values of column \"Gender\", compute the admission rate and compare them against each other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f038e712-b3d7-4e28-a809-8506d0ff8847",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dc2a3b7-659b-4710-91b1-230e19206a82",
   "metadata": {},
   "source": [
    "**Your answer:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20439634-fed0-47fa-9ca6-68c9cf001f7c",
   "metadata": {},
   "source": [
    "**d)** For each value combination of the \"Gender\" and \"Major\" columns, compute the admission rate. Compare the admission rate of women against the admission rate of men for each of the majors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "480b4533-2741-4433-a74b-8ec7bb4bb494",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9daa585b-c647-499b-9afb-78e1c3a2502e",
   "metadata": {},
   "source": [
    "**Your answer:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78625577-641f-4f34-a633-bae183fee886",
   "metadata": {},
   "source": [
    "**e)** Can you confirm there is a sex bias in the admission rates of the students?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acf0d7eb-e486-4b0e-96e9-da24e9bc8b12",
   "metadata": {},
   "source": [
    "**Your answer:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "927712ec-fd3c-4561-8732-c3babeb514f3",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Question 7: Big Data (15pts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e67686c-0788-4b91-bcf0-70b8e8738aab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "193ba79e-e377-41e0-84b1-d61fc81f0bb1",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "You are a data scientist at NASA, an agency for civil space programs, and with your team you develop and maintain the software of the NASA Crew Exploration Vehicle (CEV).  Your task is to analyze the performance of the software, and as a first exploratory step, you would like to **compute the mean execution times** of function calls within that software. Since the running vehicle will generate a high throughput of observable events in a stream, you decide to set up a MapReduce pipeline in Hadoop. \n",
    "\n",
    "The file **nasa-cev-software-tests.tsv** records timestamped events of the vehicle's software tests. The log contains the columns *Case*, *Activity* and *Timestamp*, denoting the case ID, the activity key (method call) and timestamp of the event record in nanoseconds, respectively. Furthermore, the log contains the columns *Lifecycle Transition* and *Execution ID*. The lifecycle transition takes either of the values *start* and *complete*, stating whether the corresponding activity (method call) in that row is being started or completed at the specified timestamp. The execution ID relates each event to a concrete method call, i.e., for each execution ID, there are exactly two entries (namely a *start* and a *complete* event) in the log."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5686ba3-56fa-4e35-9423-5374e47e7acf",
   "metadata": {
    "tags": []
   },
   "source": [
    "### a) Plan the Maths (2pt):"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c24cce6",
   "metadata": {},
   "source": [
    "The mean $\\mu_n$ over numerical values $v_1,...,v_n$ is well-known to be computed as $\\mu_n = \\frac{1}{n}\\sum_{i=1}^{n} v_i$.\\\n",
    "One may also use the alternative recursive formalization $\\mu_{n+m} = \\frac{n\\cdot\\mu_{n} + m\\cdot\\mu_{m}}{n+m}$. \\\n",
    "What is the advantage of using the alternative formalization when you think of handling streaming data or distributed data? Briefly explain.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4816e8fb-3968-4f34-8307-a2544fd551dc",
   "metadata": {},
   "source": [
    "**Your answer:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76ab1e45-757c-4dc4-a962-882fb3d6d95f",
   "metadata": {
    "tags": []
   },
   "source": [
    "### b) Set up MapReduce (10pts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d10d054f-512e-49a6-b935-85c80880368b",
   "metadata": {
    "tags": []
   },
   "source": [
    "Realize the computation of the mean execution times of activities as a MapReduce job. \n",
    "You need to implement this a cascaded MapReduced job. This means that the output of the first job will serve as the input of the second job. In the first job, derive the execution times of each activity execution, i.e. the time difference between the *complete* and the *start* lifecycle transition of each activity execution. In the second step, aggregate this timing information to compute the mean."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6653ff4-8c18-44d4-9de5-98ad8715425a",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### **b(i)** \n",
    "Specify the *function signatures* of the map functions and the reduce functions that you are going to use.\\\n",
    "I.e., find concrete sets to substitute $K_1, V_1, ... $ in the general signatures for map and reduce functions \\\n",
    "\n",
    "$ map:  K_1 \\times V_1 \\rightarrow (K_2 \\times V_2)^* $\\\n",
    "$ reduce: K_2 \\times (V_2)^* \\rightarrow (V_3)^*$ (or a singleton $V_3$) \n",
    "\n",
    "*Hint: You may introduce symbols to denote sets, e.g. $Act$ for the set of activities.\\\n",
    "You may also first implement the solution (b(ii)) to get an idea about the underlying signatures.\\\n",
    "Mind that you need two map and two reduce functions.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d24616b-2f62-4317-950c-20956c8a50bc",
   "metadata": {
    "tags": []
   },
   "source": [
    "**Your answer:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7511e48-b42e-4236-8232-aef6fccec140",
   "metadata": {},
   "source": [
    "#### **b(ii)**: \n",
    "Specify map functions and reduce functions to compute the mean execution time per activity as python scripts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b833afd8-718b-41aa-b54b-4193404477fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code (nasa_mapper1.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eac87097-d200-4871-a3a3-513ea6a635a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code (nasa_reducer1.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "191be80c-2ba2-45af-90fa-cff451bb656d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code (nasa_mapper2.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f15c173-dd64-4d9f-b477-c20e16ae265f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code (nasa_reducer2.py)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b2739a1-c55f-4e7e-b0d2-12bcee4f3333",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9109b6f2-cc6f-41b1-8274-f651f7b663bc",
   "metadata": {
    "tags": []
   },
   "source": [
    "### c) Run MapReduce (3pts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0de07da6-979f-4f2a-a331-843259636a0c",
   "metadata": {},
   "source": [
    "In the following, please use one of your team members' matriculation number as an identifier."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94b8fbac-5766-420a-9595-423f0b092d73",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### **c(i) (Randomization)**: \n",
    "Before applying your functions from the previous step to the dataset, please insert the matriculation number and run the following lines to randomly filter out a few of the traces in the event log, and continue working with the filtered log. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e139cb0-b347-46be-a070-ae9447fe010a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your matriculation number here\n",
    "matr_nb = # ... #\n",
    "\n",
    "# utility code (do not change)\n",
    "import random\n",
    "random.seed(matr_nb)\n",
    "\n",
    "full_df = pd.read_csv(\"datasets/nasa-cev-software-tests.tsv\", sep=\"\\t\")\n",
    "\n",
    "case_ids = list(set(full_df[\"Case\"].values))\n",
    "case_ids.sort()\n",
    "filtered_out_case_ids = random.sample(case_ids, 10)\n",
    "filtered_case_ids = [case_id for case_id in case_ids if case_id not in filtered_out_case_ids]\n",
    "randomized_df = full_df[full_df[\"Case\"].isin(filtered_case_ids)]\n",
    "\n",
    "randomized_df.to_csv(\"datasets/nasa-cev-software-tests-randomized-\" + str(matr_nb) + \".tsv\",\n",
    "          columns=[\"Activity\", \"Timestamp\", \"Lifecycle Transition\", \"Execution ID\"],\n",
    "          sep=\"\\t\", index=False, header=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aa8989a-1d2e-4360-888a-732a269525dd",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### **c(ii)**:\n",
    "Now, it is time to work with the Hadoop Distributed File System (HDFS). Follow the instructions below and show your results in each step (screenshots of the command line).\n",
    "\n",
    "    1) Import the event log to your Docker engine (at /usr/local/hadoop/(your_matr_nb)-event-log/). You also need to import the python scripts, but only document the event log import here.\n",
    "    2) Upload the files to the running HDFS (at /input/(your_matr_nb)-event-log/).\n",
    "\t3) Run Hadoop commands for the MapReduce computation.\n",
    "    4) Show the final output (computed mean execution times)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d3b0d41-bf64-43a8-a297-94da3a389f37",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "# your code\n",
    "# Image(filename='filename_screenshot_of_a1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ca0a837-d297-4bb9-b377-d9744764fc52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code\n",
    "# Image(filename='filename_screenshot_of_a2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fcbd059-5813-4bf8-95ca-a5a06c3e32c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code\n",
    "# Image(filename='filename_screenshot_of_a3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7141831-06ce-41a8-a6e2-c31de6e775af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code\n",
    "# Image(filename='filename_screenshot_of_a4')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env-IDS2022-23",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "a7934f71c1fe1a8ffe440dc1c0748dda5d23c257645054e900682c9e1d43d76a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
